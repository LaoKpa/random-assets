{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firmai/random-assets/blob/master/Linear_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-5xTpxclAKT",
        "colab_type": "code",
        "outputId": "c1e26ebd-9997-4545-f08f-f06ed272fda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/e2/4050c2e68639adf0f8a8a4857f234b71fe1a1139e25ff17575c935f49615/shap-0.33.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 17.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (0.25.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.28.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.33.0-cp36-cp36m-linux_x86_64.whl size=382248 sha256=5741555ca4e318f13cda7a9685cd73fad1ad606548c2345d5598a3e1344551fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/0f/88/a8124d43431284e10f263ffe449e119344c6145c3a165d186c\n",
            "Successfully built shap\n",
            "Installing collected packages: shap\n",
            "Successfully installed shap-0.33.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9zqd1BV5qPB",
        "colab_type": "code",
        "outputId": "4f71510a-76ad-4dea-e0ce-358da205df2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd \n",
        "import shap\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "X_pr = pd.read_csv(\"https://github.com/firmai/random-assets/blob/master/data_clean.csv?raw=true\").set_index(\"Unnamed: 0\", drop=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy9GLflalIsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pr.to_csv(\"frame.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5CD-1NufeKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_pr = pd.read_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNRijX-h8z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4hIZa87FN8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aaeada2e-0fa2-4c7e-e222-4c2352468e0d"
      },
      "source": [
        "int(first.shape[1]/11)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsqLCMM8FTYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "367346cb-9c7d-47f7-c387-2d057ea5a834"
      },
      "source": [
        "4*8"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNqEBTeskiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pr.index.name = 'index'\n",
        "target = \"price\"\n",
        "first = X_pr.sample(int(len(X_pr)/2))\n",
        "second = X_pr[~X_pr.isin(first)].dropna()\n",
        "\n",
        "\n",
        "def modelling(first, second, target, inter_portion=0.5, sqr_portion=0.5, contribution_portion=0.9, final_contribution=0.9, node_size=1.0, final=False, modeller=\"LightGBM\"):\n",
        "  \n",
        "  cols_drop = [target ]\n",
        "\n",
        "  def shap_frame(first,second, target, node_size):\n",
        "\n",
        "    #could also use linear booster XGBoost\n",
        "    d_train = lgb.Dataset(first.drop(columns=[target]), label=first[target])\n",
        "    d_valid = lgb.Dataset(second.drop(columns=[target]), label=second[target])\n",
        "    params = {\n",
        "      \n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmsle',\n",
        "        'max_depth': 6, \n",
        "        'learning_rate': 0.1,\n",
        "        'verbose': 0,\n",
        "      'num_threads':16}\n",
        "    n_estimators = 100\n",
        "\n",
        "    model = lgb.train(params, d_train, 100, verbose_eval=1)\n",
        "\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(first.drop([target], axis=1))\n",
        "    shap_fram = pd.DataFrame(shap_values[:,:], columns=list(first.drop([target], axis=1).columns))\n",
        "    shap_new = shap_fram.sum().sort_values().to_frame()\n",
        "    print(\"Finished TreeExplainer\")\n",
        "    \n",
        "    return shap_new, explainer, model\n",
        "\n",
        "  def shap_frame_keras(first,second, target, node_size):\n",
        "    ## deepexplainer and gradientexplainer has no interaction_values\n",
        "\n",
        "    # Set the input shape\n",
        "    input_shape = (len(first.columns)-1,)\n",
        "    print(f'Feature shape: {input_shape}')\n",
        "\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(int(int(first.shape[1]/11)*node_size/(contribution_portion/8)), input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(int((int(first.shape[1]/11)*node_size/2)/(contribution_portion/8)), activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "    # Configure the model and start training\n",
        "    # validation_data=(X_test, Y_test)\n",
        "\n",
        "\n",
        "    model.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.01), metrics=['mean_squared_error'])\n",
        "\n",
        "    stoppy = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "    model.fit(first.drop(columns=[target]), first[target], epochs=1000, batch_size=10, verbose=1, validation_split=0.2, callbacks=[stoppy])\n",
        "\n",
        "    #explainer = shap.DeepExplainer(model,first.drop(columns=[target]).values)\n",
        "\n",
        "    explainer  = shap.GradientExplainer(model,first.drop(columns=[target]).values)\n",
        "\n",
        "    shap_values = explainer.shap_values(first.drop([target], axis=1).values)\n",
        "    shap_fram = pd.DataFrame(shap_values[0][:,:], columns=list(first.drop([target], axis=1).columns))\n",
        "    shap_new = shap_fram.sum().sort_values().to_frame()\n",
        "\n",
        "    return shap_new, explainer, model\n",
        "\n",
        "  if modeller==\"Keras\":\n",
        "    shape_frame_all = shap_frame_keras\n",
        "  else:\n",
        "    shape_frame_all = shap_frame\n",
        "\n",
        "  shap_new, explainer, model = shape_frame_all(first,second,target,node_size)\n",
        "\n",
        "  shap_new_abs = shap_new[0].abs()\n",
        "  shap_new_abs = shap_new_abs.sort_values(ascending=False)\n",
        "  main_ft = shap_new_abs[shap_new_abs.cumsum().sub((shap_new_abs.sum()*sqr_portion)).le(0)]\n",
        "\n",
        "  preds = model.predict(second.drop(columns=[target]))\n",
        "  mse = mean_squared_error(second[target], preds)\n",
        "  print(mse)\n",
        "\n",
        "  def main_calc(new_df,main_ft):\n",
        "    df_square = new_df[list(main_ft.index)]\n",
        "    sqr_name = [fa+\"_POWER_2\" for fa in df_square.columns]\n",
        "    log_p_name = [fa+\"_LOG_p_one_abs\" for fa in df_square.columns]\n",
        "    rec_p_name = [fa+\"_RECIP_p_one\" for fa in df_square.columns]\n",
        "    sqrt_name = [fa+\"_SQRT_p_one\" for fa in df_square.columns]\n",
        "\n",
        "    df_sqr = pd.DataFrame(np.power(df_square.values, 2),columns=sqr_name, index=new_df.index)\n",
        "    df_log = pd.DataFrame(np.log(df_square.add(1).abs().values),columns=log_p_name, index=new_df.index)\n",
        "    df_rec = pd.DataFrame(np.reciprocal(df_square.add(1).values),columns=rec_p_name, index=new_df.index)\n",
        "    df_sqrt = pd.DataFrame(np.sqrt(df_square.abs().add(1).values),columns=sqrt_name, index=new_df.index)\n",
        "\n",
        "    dfs = [df_sqr, df_log, df_rec, df_sqrt]\n",
        "\n",
        "    df_connect=  pd.concat(dfs, axis=1)\n",
        "\n",
        "    return df_connect\n",
        "\n",
        "  ## An attempt to decrease the amount of interactin features.\n",
        "\n",
        "  select_ft = shap_new_abs[shap_new_abs.cumsum().sub((shap_new_abs.sum()*contribution_portion)).le(0)]\n",
        "  select_ft = list(select_ft.index)\n",
        "  select_ft.append(target)\n",
        "\n",
        "  ## has to remain tree for interaction effects. shap_frame\n",
        "  shap_select, explainer, model = shap_frame(first[select_ft], second[select_ft], target, node_size )\n",
        "  shap_select_abs = shap_select[0].abs()\n",
        "\n",
        "  ### Interactions Features\n",
        "  shap_interaction_values = explainer.shap_interaction_values(first[select_ft].drop(cols_drop, axis=1))\n",
        "\n",
        "  shap_interaction_values_abs = abs(shap_interaction_values)\n",
        "  df_start = pd.DataFrame(np.sum(shap_interaction_values_abs ,axis=0),columns=first[select_ft].drop(cols_drop, axis=1).columns, index=first[select_ft].drop(cols_drop, axis=1).columns)\n",
        "\n",
        "  #the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
        "  sol = (df_start.where(np.triu(np.ones(df_start.shape), k=1).astype(np.bool))\n",
        "                  .stack()\n",
        "                  .sort_values(ascending=False))\n",
        "  #first element of sol series is the pair with the bigest correlation\n",
        "\n",
        "  ## New Data Frames From Feature Interaction and Main Feature\n",
        "  ## If you have features [a, b, c] the default polynomial features(in sklearn the degree is 2) should be [1, a, b, c, a^2, b^2, c^2, ab, bc, ca].\n",
        "\n",
        "  ## Interaction Calculations\n",
        "\n",
        "  dab = sol[sol.cumsum().sub((sol.sum()*inter_portion)).le(0)]\n",
        "\n",
        "  list_one = [da[0] for da in dab.index]\n",
        "  list_two = [da[1] for da in dab.index]\n",
        "\n",
        "  def inter_cal(list_one, list_two,new_df):\n",
        "\n",
        "    mult = [ra+\"_X_\"+ba for ra, ba in zip(list_one, list_two)]\n",
        "    div = [ra+\"_DIV_\"+ba for ra, ba in zip(list_one, list_two)]\n",
        "    print(\"len one \" + str(len(list_one)) )\n",
        "    print(\"len two \" + str(len(list_two)) )\n",
        "    inter_mult = pd.DataFrame(new_df[list_one].values*new_df[list_two].values, columns=mult, index=new_df.index)\n",
        "    div_p_one = pd.DataFrame(new_df[list_one].add(1).values/new_df[list_two].add(1).values, columns=div, index=new_df.index)\n",
        "\n",
        "    df_one = pd.concat((inter_mult,div_p_one), axis=1)\n",
        "\n",
        "    return df_one\n",
        "\n",
        "\n",
        "  def combine(target, list_one, list_two, main_ft, new_df):\n",
        "\n",
        "    inter_mult = inter_cal(list_one, list_two, new_df.drop(columns=[target]))\n",
        "\n",
        "    df_sqr = main_calc(new_df.drop(columns=[target]),main_ft)\n",
        "\n",
        "    new = pd.concat((inter_mult,df_sqr),axis=1)\n",
        "    new2 = pd.concat((new_df,new),axis=1)\n",
        "\n",
        "    new2 = new2.loc[:,~new2.columns.duplicated()]\n",
        "\n",
        "    return new2\n",
        "\n",
        "  new_first = combine(target, list_one, list_two, main_ft, first)\n",
        "\n",
        "  new_second = combine(target, list_one, list_two, main_ft, second)\n",
        "\n",
        "  if final:\n",
        "    print(\"final\")\n",
        "    shap_select, explainer, model = shape_frame_all(new_first, new_second,target,node_size )\n",
        "    ### Can be put into function, somewhat unnecessary\n",
        "    shap_select_abs = shap_select[0].abs()\n",
        "    shap_select_abs = shap_select_abs.sort_values(ascending=False)\n",
        "    final_ft = shap_select_abs[shap_select_abs.cumsum().sub((shap_select_abs.sum()*final_contribution)).le(0)]\n",
        "    final_ft = list(final_ft.index)\n",
        "    final_ft.append(target)\n",
        "    return new_first,new_second, mse,final_ft, shap_select_abs\n",
        "\n",
        "  return new_first,new_second, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT21QuoEHJDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "## Definitely a benefit to run twice but not three times. \n",
        "def runner(first_run, second_run, target, inter_portion=0.8, sqr_portion=0.9, contribution_portion=0.9, final_contribution=0.95, deflator=0.7, node_size=1.0 , runs=2, modeller=\"LightGBM\"):\n",
        "#def runner(first_run, second_run, target, inter_portion=0.4, sqr_portion=0.3, contribution_portion=0.9, deflator=0.6, runs=3):\n",
        "  for r in range(runs):\n",
        "    r += 1\n",
        "    if r ==runs:\n",
        "      print(\"final\")\n",
        "      first_run, second_run, mse, shapper, shap_select_abs = modelling(first_run, second_run, target, inter_portion, sqr_portion, contribution_portion, final_contribution,node_size, True, modeller)\n",
        "    else:\n",
        "      first_run, second_run, mse = modelling(first_run, second_run, target, inter_portion, sqr_portion, contribution_portion, final_contribution, node_size, False, modeller)\n",
        "\n",
        "    inter_portion = inter_portion * deflator\n",
        "    sqr_portion = sqr_portion * deflator * 1.1\n",
        "    contribution_portion = contribution_portion * deflator\n",
        "    gc.collect()\n",
        "  \n",
        "  return first_run, second_run, shapper, shap_select_abs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SArcKbhwh0h9",
        "colab_type": "code",
        "outputId": "18ed375a-1741-4da2-bf71-0a80c771b9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "new_first_deep, new_second_deep, shapper_deep, shap_select_abs_deep = runner(first, second, target, modeller=\"Keras\")"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature shape: (53,)\n",
            "Train on 4180 samples, validate on 1046 samples\n",
            "Epoch 1/100\n",
            "4180/4180 [==============================] - 1s 203us/step - loss: 86.7481 - mean_squared_error: 25311.0536 - val_loss: 77.3674 - val_mean_squared_error: 19551.2793\n",
            "Epoch 2/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 76.7712 - mean_squared_error: 21917.7316 - val_loss: 68.8881 - val_mean_squared_error: 17909.7802\n",
            "Epoch 3/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 71.6620 - mean_squared_error: 19328.0037 - val_loss: 67.1379 - val_mean_squared_error: 17561.8642\n",
            "Epoch 4/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 71.4044 - mean_squared_error: 19126.8401 - val_loss: 64.8086 - val_mean_squared_error: 15749.8226\n",
            "Epoch 5/100\n",
            "4180/4180 [==============================] - 1s 141us/step - loss: 69.4694 - mean_squared_error: 18455.2205 - val_loss: 67.0606 - val_mean_squared_error: 15105.6256\n",
            "Epoch 6/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 68.9877 - mean_squared_error: 17313.2399 - val_loss: 70.8026 - val_mean_squared_error: 20425.2998\n",
            "Epoch 7/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 67.4143 - mean_squared_error: 17413.2958 - val_loss: 64.5990 - val_mean_squared_error: 17188.3157\n",
            "Epoch 8/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 67.9287 - mean_squared_error: 17271.5384 - val_loss: 66.5424 - val_mean_squared_error: 16907.7511\n",
            "Epoch 9/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 67.4772 - mean_squared_error: 16776.1621 - val_loss: 61.2591 - val_mean_squared_error: 14683.5503\n",
            "Epoch 10/100\n",
            "4180/4180 [==============================] - 1s 143us/step - loss: 66.0244 - mean_squared_error: 16487.0756 - val_loss: 64.6081 - val_mean_squared_error: 15898.8912\n",
            "Epoch 11/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 67.6177 - mean_squared_error: 17304.4145 - val_loss: 60.2419 - val_mean_squared_error: 13017.5551\n",
            "Epoch 12/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 66.2154 - mean_squared_error: 16282.6741 - val_loss: 63.0723 - val_mean_squared_error: 13962.1682\n",
            "Epoch 13/100\n",
            "4180/4180 [==============================] - 1s 143us/step - loss: 65.1460 - mean_squared_error: 15739.5704 - val_loss: 60.0774 - val_mean_squared_error: 12734.3317\n",
            "Epoch 14/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 66.0800 - mean_squared_error: 15983.2416 - val_loss: 65.2046 - val_mean_squared_error: 16643.6580\n",
            "Epoch 15/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 64.5230 - mean_squared_error: 15360.5609 - val_loss: 68.4022 - val_mean_squared_error: 17278.3511\n",
            "Epoch 16/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 65.4207 - mean_squared_error: 15967.5552 - val_loss: 64.1825 - val_mean_squared_error: 14975.8421\n",
            "Epoch 17/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 64.2368 - mean_squared_error: 15362.3781 - val_loss: 61.1436 - val_mean_squared_error: 12717.5516\n",
            "Epoch 18/100\n",
            "4180/4180 [==============================] - 1s 148us/step - loss: 64.3830 - mean_squared_error: 15430.8958 - val_loss: 61.0021 - val_mean_squared_error: 12215.9250\n",
            "Epoch 19/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 64.3077 - mean_squared_error: 15427.6060 - val_loss: 60.8517 - val_mean_squared_error: 12989.7953\n",
            "Epoch 20/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 64.9258 - mean_squared_error: 15439.5721 - val_loss: 59.7059 - val_mean_squared_error: 12808.8655\n",
            "Epoch 21/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 63.8460 - mean_squared_error: 15166.0604 - val_loss: 62.3124 - val_mean_squared_error: 13613.5638\n",
            "Epoch 22/100\n",
            "4180/4180 [==============================] - 1s 130us/step - loss: 63.6723 - mean_squared_error: 15533.8351 - val_loss: 59.8604 - val_mean_squared_error: 12327.9668\n",
            "Epoch 23/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 63.9630 - mean_squared_error: 15270.3756 - val_loss: 60.7435 - val_mean_squared_error: 12094.3458\n",
            "Epoch 24/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 64.1432 - mean_squared_error: 15360.4059 - val_loss: 63.2529 - val_mean_squared_error: 11858.5669\n",
            "Epoch 25/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 63.4527 - mean_squared_error: 14876.1654 - val_loss: 59.6036 - val_mean_squared_error: 11522.7032\n",
            "Epoch 26/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 63.0925 - mean_squared_error: 14683.8213 - val_loss: 61.8957 - val_mean_squared_error: 13324.1031\n",
            "Epoch 27/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 63.2153 - mean_squared_error: 15210.5147 - val_loss: 60.6924 - val_mean_squared_error: 13109.7021\n",
            "Epoch 28/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 63.0031 - mean_squared_error: 14021.1835 - val_loss: 61.5043 - val_mean_squared_error: 13038.9692\n",
            "Epoch 29/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 62.7719 - mean_squared_error: 14151.8637 - val_loss: 60.1953 - val_mean_squared_error: 12359.7935\n",
            "Epoch 30/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 63.0026 - mean_squared_error: 15152.5022 - val_loss: 61.8145 - val_mean_squared_error: 13068.0409\n",
            "Epoch 31/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 62.7840 - mean_squared_error: 14779.3179 - val_loss: 61.0501 - val_mean_squared_error: 13032.1769\n",
            "Epoch 32/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 62.6006 - mean_squared_error: 14479.5402 - val_loss: 60.7357 - val_mean_squared_error: 11213.3189\n",
            "Epoch 33/100\n",
            "4180/4180 [==============================] - 1s 129us/step - loss: 61.8534 - mean_squared_error: 14215.0286 - val_loss: 61.4329 - val_mean_squared_error: 14452.5601\n",
            "Epoch 34/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 62.3354 - mean_squared_error: 14149.3259 - val_loss: 59.4380 - val_mean_squared_error: 12117.9672\n",
            "Epoch 35/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 62.1126 - mean_squared_error: 13992.6652 - val_loss: 59.6308 - val_mean_squared_error: 13266.7561\n",
            "Epoch 36/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 62.4948 - mean_squared_error: 14710.3861 - val_loss: 61.8434 - val_mean_squared_error: 14574.2836\n",
            "Epoch 37/100\n",
            "4180/4180 [==============================] - 1s 130us/step - loss: 62.0012 - mean_squared_error: 14605.6264 - val_loss: 61.3052 - val_mean_squared_error: 13219.6146\n",
            "Epoch 38/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 62.5301 - mean_squared_error: 14746.8552 - val_loss: 64.1874 - val_mean_squared_error: 16400.0059\n",
            "Epoch 39/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 62.1003 - mean_squared_error: 14780.4326 - val_loss: 62.3206 - val_mean_squared_error: 11617.3361\n",
            "Epoch 40/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 62.1606 - mean_squared_error: 14487.9670 - val_loss: 58.7505 - val_mean_squared_error: 11582.6888\n",
            "Epoch 41/100\n",
            "4180/4180 [==============================] - 1s 144us/step - loss: 62.6744 - mean_squared_error: 14865.6503 - val_loss: 64.1906 - val_mean_squared_error: 11534.6967\n",
            "Epoch 42/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 62.7973 - mean_squared_error: 14726.3139 - val_loss: 60.2430 - val_mean_squared_error: 12430.9942\n",
            "Epoch 43/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.8178 - mean_squared_error: 14699.1010 - val_loss: 59.5542 - val_mean_squared_error: 12208.2402\n",
            "Epoch 44/100\n",
            "4180/4180 [==============================] - 1s 145us/step - loss: 62.5668 - mean_squared_error: 14488.2772 - val_loss: 61.9005 - val_mean_squared_error: 12570.4295\n",
            "Epoch 45/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 60.9674 - mean_squared_error: 14122.6835 - val_loss: 60.6261 - val_mean_squared_error: 13882.9901\n",
            "Epoch 46/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 61.8498 - mean_squared_error: 14022.8606 - val_loss: 59.2558 - val_mean_squared_error: 12621.9248\n",
            "Epoch 47/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 61.4411 - mean_squared_error: 14233.0311 - val_loss: 60.6944 - val_mean_squared_error: 12634.0080\n",
            "Epoch 48/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.6707 - mean_squared_error: 14102.3497 - val_loss: 60.2789 - val_mean_squared_error: 13546.8340\n",
            "Epoch 49/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 61.2450 - mean_squared_error: 14147.1129 - val_loss: 59.8649 - val_mean_squared_error: 13014.0718\n",
            "Epoch 50/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.3714 - mean_squared_error: 14168.1259 - val_loss: 62.3134 - val_mean_squared_error: 11954.5349\n",
            "Epoch 51/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 63.1286 - mean_squared_error: 14882.7284 - val_loss: 59.1884 - val_mean_squared_error: 12132.1948\n",
            "Epoch 52/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.9094 - mean_squared_error: 14545.7480 - val_loss: 60.5016 - val_mean_squared_error: 11213.3775\n",
            "Epoch 53/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.6109 - mean_squared_error: 13959.0453 - val_loss: 58.6304 - val_mean_squared_error: 12434.2191\n",
            "Epoch 54/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 60.9025 - mean_squared_error: 14162.3134 - val_loss: 58.9969 - val_mean_squared_error: 11962.2349\n",
            "Epoch 55/100\n",
            "4180/4180 [==============================] - 1s 130us/step - loss: 61.2350 - mean_squared_error: 14162.3134 - val_loss: 60.8278 - val_mean_squared_error: 13368.9600\n",
            "Epoch 56/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 61.3374 - mean_squared_error: 14146.8886 - val_loss: 60.8347 - val_mean_squared_error: 11838.7733\n",
            "Epoch 57/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 61.3446 - mean_squared_error: 14121.8734 - val_loss: 60.4438 - val_mean_squared_error: 13148.7063\n",
            "Epoch 58/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 60.3988 - mean_squared_error: 13740.0848 - val_loss: 63.6431 - val_mean_squared_error: 12980.1061\n",
            "Epoch 59/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 61.1573 - mean_squared_error: 13921.4376 - val_loss: 59.3436 - val_mean_squared_error: 12131.8171\n",
            "Epoch 60/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.3608 - mean_squared_error: 13493.2984 - val_loss: 59.3720 - val_mean_squared_error: 12266.6989\n",
            "Epoch 61/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 60.9697 - mean_squared_error: 13772.9382 - val_loss: 59.6875 - val_mean_squared_error: 12798.2202\n",
            "Epoch 62/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 61.2347 - mean_squared_error: 14097.6495 - val_loss: 59.6393 - val_mean_squared_error: 12010.3816\n",
            "Epoch 63/100\n",
            "4180/4180 [==============================] - 1s 136us/step - loss: 61.2785 - mean_squared_error: 14230.0095 - val_loss: 59.7458 - val_mean_squared_error: 12897.7010\n",
            "Epoch 64/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 61.4170 - mean_squared_error: 13968.4370 - val_loss: 60.4355 - val_mean_squared_error: 11651.8812\n",
            "Epoch 65/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.1852 - mean_squared_error: 14539.8792 - val_loss: 62.8402 - val_mean_squared_error: 11742.9890\n",
            "Epoch 66/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 62.0009 - mean_squared_error: 14729.2684 - val_loss: 59.0982 - val_mean_squared_error: 11105.6161\n",
            "Epoch 67/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 61.2876 - mean_squared_error: 14184.5640 - val_loss: 59.8498 - val_mean_squared_error: 13072.7243\n",
            "Epoch 68/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.7593 - mean_squared_error: 13834.0969 - val_loss: 59.9203 - val_mean_squared_error: 11851.0316\n",
            "Epoch 69/100\n",
            "4180/4180 [==============================] - 1s 138us/step - loss: 61.0425 - mean_squared_error: 13738.5185 - val_loss: 65.3523 - val_mean_squared_error: 13933.7965\n",
            "Epoch 70/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 61.1195 - mean_squared_error: 14119.2072 - val_loss: 59.4651 - val_mean_squared_error: 12861.3743\n",
            "Epoch 71/100\n",
            "4180/4180 [==============================] - 1s 143us/step - loss: 60.7088 - mean_squared_error: 13905.8939 - val_loss: 60.9215 - val_mean_squared_error: 11402.9803\n",
            "Epoch 72/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 61.5034 - mean_squared_error: 14529.0114 - val_loss: 60.9291 - val_mean_squared_error: 13257.8501\n",
            "Epoch 73/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.6697 - mean_squared_error: 13795.2451 - val_loss: 58.5801 - val_mean_squared_error: 11468.8968\n",
            "Epoch 74/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 60.5930 - mean_squared_error: 13863.0716 - val_loss: 59.6089 - val_mean_squared_error: 11206.2348\n",
            "Epoch 75/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 60.4571 - mean_squared_error: 13683.6637 - val_loss: 59.2065 - val_mean_squared_error: 12453.6223\n",
            "Epoch 76/100\n",
            "4180/4180 [==============================] - 1s 130us/step - loss: 60.6715 - mean_squared_error: 14049.3259 - val_loss: 60.4200 - val_mean_squared_error: 11620.6287\n",
            "Epoch 77/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.0121 - mean_squared_error: 14483.8606 - val_loss: 59.2873 - val_mean_squared_error: 11710.9581\n",
            "Epoch 78/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 60.6291 - mean_squared_error: 14092.3718 - val_loss: 60.4589 - val_mean_squared_error: 11676.1549\n",
            "Epoch 79/100\n",
            "4180/4180 [==============================] - 1s 142us/step - loss: 60.6497 - mean_squared_error: 13893.0154 - val_loss: 59.5202 - val_mean_squared_error: 11845.0607\n",
            "Epoch 80/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 60.7892 - mean_squared_error: 13798.6773 - val_loss: 59.3967 - val_mean_squared_error: 11283.0362\n",
            "Epoch 81/100\n",
            "4180/4180 [==============================] - 1s 131us/step - loss: 61.2895 - mean_squared_error: 14381.8342 - val_loss: 59.4170 - val_mean_squared_error: 12697.9570\n",
            "Epoch 82/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 61.2413 - mean_squared_error: 13927.2932 - val_loss: 59.2263 - val_mean_squared_error: 11428.9613\n",
            "Epoch 83/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 61.5135 - mean_squared_error: 14450.9281 - val_loss: 60.5347 - val_mean_squared_error: 12188.3603\n",
            "Epoch 84/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.6062 - mean_squared_error: 13723.5764 - val_loss: 63.5305 - val_mean_squared_error: 11734.0562\n",
            "Epoch 85/100\n",
            "4180/4180 [==============================] - 1s 142us/step - loss: 59.9608 - mean_squared_error: 13374.4210 - val_loss: 59.0443 - val_mean_squared_error: 12445.8961\n",
            "Epoch 86/100\n",
            "4180/4180 [==============================] - 1s 144us/step - loss: 60.3308 - mean_squared_error: 13529.0812 - val_loss: 58.3359 - val_mean_squared_error: 12011.3638\n",
            "Epoch 87/100\n",
            "4180/4180 [==============================] - 1s 145us/step - loss: 60.1830 - mean_squared_error: 13472.9391 - val_loss: 62.9407 - val_mean_squared_error: 15028.3451\n",
            "Epoch 88/100\n",
            "4180/4180 [==============================] - 1s 153us/step - loss: 60.8792 - mean_squared_error: 13789.8133 - val_loss: 58.9363 - val_mean_squared_error: 10709.5358\n",
            "Epoch 89/100\n",
            "4180/4180 [==============================] - 1s 146us/step - loss: 60.6174 - mean_squared_error: 13925.8135 - val_loss: 58.8996 - val_mean_squared_error: 10723.7420\n",
            "Epoch 90/100\n",
            "4180/4180 [==============================] - 1s 147us/step - loss: 60.3769 - mean_squared_error: 13955.9195 - val_loss: 58.3547 - val_mean_squared_error: 11421.7990\n",
            "Epoch 91/100\n",
            "4180/4180 [==============================] - 1s 142us/step - loss: 61.0003 - mean_squared_error: 14052.3703 - val_loss: 61.8543 - val_mean_squared_error: 14987.5559\n",
            "Epoch 92/100\n",
            "4180/4180 [==============================] - 1s 147us/step - loss: 60.7570 - mean_squared_error: 13924.0197 - val_loss: 60.5930 - val_mean_squared_error: 12211.8606\n",
            "Epoch 93/100\n",
            "4180/4180 [==============================] - 1s 146us/step - loss: 60.3530 - mean_squared_error: 13880.1751 - val_loss: 61.4033 - val_mean_squared_error: 14344.4004\n",
            "Epoch 94/100\n",
            "4180/4180 [==============================] - 1s 137us/step - loss: 60.1374 - mean_squared_error: 13796.1870 - val_loss: 58.6273 - val_mean_squared_error: 11464.7769\n",
            "Epoch 95/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 60.2833 - mean_squared_error: 13411.2255 - val_loss: 60.3655 - val_mean_squared_error: 13210.2563\n",
            "Epoch 96/100\n",
            "4180/4180 [==============================] - 1s 132us/step - loss: 59.9764 - mean_squared_error: 13369.4535 - val_loss: 58.6462 - val_mean_squared_error: 10897.2291\n",
            "Epoch 97/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 59.9348 - mean_squared_error: 13593.5019 - val_loss: 57.8145 - val_mean_squared_error: 11679.6373\n",
            "Epoch 98/100\n",
            "4180/4180 [==============================] - 1s 134us/step - loss: 60.4906 - mean_squared_error: 13821.0501 - val_loss: 58.9126 - val_mean_squared_error: 12600.1028\n",
            "Epoch 99/100\n",
            "4180/4180 [==============================] - 1s 135us/step - loss: 60.0793 - mean_squared_error: 13652.7606 - val_loss: 58.2711 - val_mean_squared_error: 10987.6742\n",
            "Epoch 100/100\n",
            "4180/4180 [==============================] - 1s 133us/step - loss: 59.7881 - mean_squared_error: 13492.7171 - val_loss: 58.7538 - val_mean_squared_error: 11909.5714\n",
            "12810.224541586178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished TreeExplainer\n",
            "len one 41\n",
            "len two 41\n",
            "len one 41\n",
            "len two 41\n",
            "final\n",
            "Feature shape: (219,)\n",
            "Train on 4180 samples, validate on 1046 samples\n",
            "Epoch 1/100\n",
            "4180/4180 [==============================] - 1s 325us/step - loss: 1250.4924 - mean_squared_error: 293927418.1353 - val_loss: 162.5181 - val_mean_squared_error: 61902.2344\n",
            "Epoch 2/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 150.2068 - mean_squared_error: 56800.3353 - val_loss: 139.7689 - val_mean_squared_error: 50363.1580\n",
            "Epoch 3/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 146.1095 - mean_squared_error: 59529.9556 - val_loss: 137.4926 - val_mean_squared_error: 48940.0446\n",
            "Epoch 4/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 142.3763 - mean_squared_error: 53100.3841 - val_loss: 138.8018 - val_mean_squared_error: 49991.9513\n",
            "Epoch 5/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 141.0518 - mean_squared_error: 58894.5004 - val_loss: 133.4132 - val_mean_squared_error: 47717.1412\n",
            "Epoch 6/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 510.6591 - mean_squared_error: 41508573.2165 - val_loss: 111.6492 - val_mean_squared_error: 37422.9268\n",
            "Epoch 7/100\n",
            "4180/4180 [==============================] - 1s 226us/step - loss: 136.4065 - mean_squared_error: 67406.7425 - val_loss: 148.9122 - val_mean_squared_error: 53724.7905\n",
            "Epoch 8/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 140.9630 - mean_squared_error: 52645.2854 - val_loss: 127.3063 - val_mean_squared_error: 45105.8090\n",
            "Epoch 9/100\n",
            "4180/4180 [==============================] - 1s 244us/step - loss: 132.6936 - mean_squared_error: 48782.9037 - val_loss: 130.2792 - val_mean_squared_error: 46847.8953\n",
            "Epoch 10/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 130.8082 - mean_squared_error: 48335.8257 - val_loss: 124.2656 - val_mean_squared_error: 44612.5590\n",
            "Epoch 11/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 142.5539 - mean_squared_error: 86033.4567 - val_loss: 124.0115 - val_mean_squared_error: 44730.0006\n",
            "Epoch 12/100\n",
            "4180/4180 [==============================] - 1s 245us/step - loss: 128.8938 - mean_squared_error: 47586.9458 - val_loss: 124.1999 - val_mean_squared_error: 45396.8660\n",
            "Epoch 13/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 125.0559 - mean_squared_error: 47171.3488 - val_loss: 120.0680 - val_mean_squared_error: 42362.2647\n",
            "Epoch 14/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 122.5099 - mean_squared_error: 44615.5594 - val_loss: 116.6444 - val_mean_squared_error: 41474.5281\n",
            "Epoch 15/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 117.8800 - mean_squared_error: 42945.5980 - val_loss: 114.7166 - val_mean_squared_error: 40078.4572\n",
            "Epoch 16/100\n",
            "4180/4180 [==============================] - 1s 239us/step - loss: 116.7835 - mean_squared_error: 42649.5809 - val_loss: 113.3593 - val_mean_squared_error: 39100.8977\n",
            "Epoch 17/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 114.4401 - mean_squared_error: 41463.4360 - val_loss: 108.5511 - val_mean_squared_error: 38458.1199\n",
            "Epoch 18/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 112.7972 - mean_squared_error: 40988.3600 - val_loss: 106.1015 - val_mean_squared_error: 37567.0826\n",
            "Epoch 19/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 111.9477 - mean_squared_error: 44182.5988 - val_loss: 112.7024 - val_mean_squared_error: 38887.0039\n",
            "Epoch 20/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 112.2241 - mean_squared_error: 41126.5031 - val_loss: 106.9436 - val_mean_squared_error: 37366.2066\n",
            "Epoch 21/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 117.7464 - mean_squared_error: 139796.9648 - val_loss: 109.4279 - val_mean_squared_error: 39310.6006\n",
            "Epoch 22/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 110.4487 - mean_squared_error: 40719.3624 - val_loss: 105.6151 - val_mean_squared_error: 36227.0819\n",
            "Epoch 23/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 123.8626 - mean_squared_error: 136129.7180 - val_loss: 107.1078 - val_mean_squared_error: 38119.2362\n",
            "Epoch 24/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 239.2539 - mean_squared_error: 13640052.8168 - val_loss: 103.4563 - val_mean_squared_error: 36831.9974\n",
            "Epoch 25/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 107.7341 - mean_squared_error: 40207.5489 - val_loss: 109.3529 - val_mean_squared_error: 38566.9764\n",
            "Epoch 26/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 106.9132 - mean_squared_error: 38718.5081 - val_loss: 101.4537 - val_mean_squared_error: 34778.5077\n",
            "Epoch 27/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 104.4807 - mean_squared_error: 37344.2152 - val_loss: 101.7169 - val_mean_squared_error: 35474.4775\n",
            "Epoch 28/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 103.1606 - mean_squared_error: 36766.7205 - val_loss: 99.0120 - val_mean_squared_error: 33878.2493\n",
            "Epoch 29/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 103.6979 - mean_squared_error: 36799.7635 - val_loss: 100.3258 - val_mean_squared_error: 33724.3178\n",
            "Epoch 30/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 101.8962 - mean_squared_error: 35878.9302 - val_loss: 100.9193 - val_mean_squared_error: 33700.8547\n",
            "Epoch 31/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 100.9732 - mean_squared_error: 35863.2142 - val_loss: 99.1461 - val_mean_squared_error: 33857.1190\n",
            "Epoch 32/100\n",
            "4180/4180 [==============================] - 1s 228us/step - loss: 101.2096 - mean_squared_error: 35643.1738 - val_loss: 101.3231 - val_mean_squared_error: 37379.0088\n",
            "Epoch 33/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 101.2032 - mean_squared_error: 35839.3862 - val_loss: 97.7304 - val_mean_squared_error: 33331.0594\n",
            "Epoch 34/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 101.1672 - mean_squared_error: 35644.7195 - val_loss: 100.0293 - val_mean_squared_error: 35020.2229\n",
            "Epoch 35/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 100.2617 - mean_squared_error: 34839.1036 - val_loss: 104.1552 - val_mean_squared_error: 36627.7238\n",
            "Epoch 36/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 101.3606 - mean_squared_error: 35302.0934 - val_loss: 96.8783 - val_mean_squared_error: 32658.5105\n",
            "Epoch 37/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 101.7164 - mean_squared_error: 35547.0564 - val_loss: 99.3069 - val_mean_squared_error: 33954.5031\n",
            "Epoch 38/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 101.8850 - mean_squared_error: 36402.5191 - val_loss: 129.0683 - val_mean_squared_error: 54255.3346\n",
            "Epoch 39/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 100.7745 - mean_squared_error: 34987.3192 - val_loss: 99.1416 - val_mean_squared_error: 31950.6823\n",
            "Epoch 40/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 99.8449 - mean_squared_error: 34078.8544 - val_loss: 96.8210 - val_mean_squared_error: 31650.2874\n",
            "Epoch 41/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 99.9772 - mean_squared_error: 34139.7997 - val_loss: 98.8607 - val_mean_squared_error: 33143.8635\n",
            "Epoch 42/100\n",
            "4180/4180 [==============================] - 1s 247us/step - loss: 99.3025 - mean_squared_error: 33899.2296 - val_loss: 94.6316 - val_mean_squared_error: 31459.8968\n",
            "Epoch 43/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 95.9115 - mean_squared_error: 33890.5167 - val_loss: 91.1751 - val_mean_squared_error: 30896.5140\n",
            "Epoch 44/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 95.3429 - mean_squared_error: 33969.0788 - val_loss: 94.9790 - val_mean_squared_error: 32917.9041\n",
            "Epoch 45/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 99.5102 - mean_squared_error: 49138.4809 - val_loss: 94.9668 - val_mean_squared_error: 33366.3927\n",
            "Epoch 46/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 99.6475 - mean_squared_error: 43520.6697 - val_loss: 90.7547 - val_mean_squared_error: 30187.4724\n",
            "Epoch 47/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 93.3559 - mean_squared_error: 32613.2345 - val_loss: 90.0983 - val_mean_squared_error: 30502.1840\n",
            "Epoch 48/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 94.0900 - mean_squared_error: 33354.1955 - val_loss: 91.8458 - val_mean_squared_error: 30985.8297\n",
            "Epoch 49/100\n",
            "4180/4180 [==============================] - 1s 229us/step - loss: 92.6989 - mean_squared_error: 32627.1735 - val_loss: 90.2337 - val_mean_squared_error: 30635.2074\n",
            "Epoch 50/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 92.5767 - mean_squared_error: 32569.2395 - val_loss: 89.8461 - val_mean_squared_error: 30582.3707\n",
            "Epoch 51/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 91.0510 - mean_squared_error: 31706.5817 - val_loss: 91.3280 - val_mean_squared_error: 30422.6458\n",
            "Epoch 52/100\n",
            "4180/4180 [==============================] - 1s 240us/step - loss: 91.4420 - mean_squared_error: 31882.1281 - val_loss: 90.7272 - val_mean_squared_error: 29812.2846\n",
            "Epoch 53/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 91.0742 - mean_squared_error: 31875.4417 - val_loss: 91.2241 - val_mean_squared_error: 30635.4133\n",
            "Epoch 54/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 91.2501 - mean_squared_error: 31686.1032 - val_loss: 89.0194 - val_mean_squared_error: 29377.0484\n",
            "Epoch 55/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 91.1012 - mean_squared_error: 31791.3854 - val_loss: 87.9378 - val_mean_squared_error: 29190.8348\n",
            "Epoch 56/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 90.9693 - mean_squared_error: 31139.5810 - val_loss: 89.1208 - val_mean_squared_error: 28654.0850\n",
            "Epoch 57/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 91.5096 - mean_squared_error: 31664.2098 - val_loss: 89.0416 - val_mean_squared_error: 30522.4620\n",
            "Epoch 58/100\n",
            "4180/4180 [==============================] - 1s 239us/step - loss: 90.9432 - mean_squared_error: 31503.7369 - val_loss: 89.2238 - val_mean_squared_error: 29271.1143\n",
            "Epoch 59/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 89.9003 - mean_squared_error: 31334.7767 - val_loss: 90.0029 - val_mean_squared_error: 30281.7055\n",
            "Epoch 60/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 90.0214 - mean_squared_error: 31346.3948 - val_loss: 88.4383 - val_mean_squared_error: 29318.3866\n",
            "Epoch 61/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 91.1943 - mean_squared_error: 31754.3179 - val_loss: 89.0879 - val_mean_squared_error: 29138.1422\n",
            "Epoch 62/100\n",
            "4180/4180 [==============================] - 1s 242us/step - loss: 91.0208 - mean_squared_error: 31772.3243 - val_loss: 88.8758 - val_mean_squared_error: 29236.8121\n",
            "Epoch 63/100\n",
            "4180/4180 [==============================] - 1s 239us/step - loss: 90.8104 - mean_squared_error: 31257.8396 - val_loss: 91.6723 - val_mean_squared_error: 31095.7126\n",
            "Epoch 64/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 90.6294 - mean_squared_error: 30877.7453 - val_loss: 87.2099 - val_mean_squared_error: 28584.9563\n",
            "Epoch 65/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 90.5544 - mean_squared_error: 32142.6210 - val_loss: 94.0986 - val_mean_squared_error: 36086.4985\n",
            "Epoch 66/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 89.3404 - mean_squared_error: 31129.2653 - val_loss: 87.6423 - val_mean_squared_error: 29431.8310\n",
            "Epoch 67/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 89.5915 - mean_squared_error: 30921.4867 - val_loss: 86.9262 - val_mean_squared_error: 28734.9841\n",
            "Epoch 68/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 89.0463 - mean_squared_error: 30346.9593 - val_loss: 86.3398 - val_mean_squared_error: 28192.7643\n",
            "Epoch 69/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 99.3607 - mean_squared_error: 205205.1092 - val_loss: 90.5718 - val_mean_squared_error: 30027.6996\n",
            "Epoch 70/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 90.4334 - mean_squared_error: 30518.0777 - val_loss: 88.8146 - val_mean_squared_error: 29775.3243\n",
            "Epoch 71/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 90.2743 - mean_squared_error: 31093.5303 - val_loss: 86.8342 - val_mean_squared_error: 29079.5374\n",
            "Epoch 72/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 89.5643 - mean_squared_error: 30938.4290 - val_loss: 88.7966 - val_mean_squared_error: 29519.6012\n",
            "Epoch 73/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 89.3420 - mean_squared_error: 30744.3428 - val_loss: 90.7857 - val_mean_squared_error: 29553.3602\n",
            "Epoch 74/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 88.6821 - mean_squared_error: 30317.0766 - val_loss: 86.0060 - val_mean_squared_error: 28098.5983\n",
            "Epoch 75/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 88.6378 - mean_squared_error: 29912.3728 - val_loss: 88.8665 - val_mean_squared_error: 29575.1229\n",
            "Epoch 76/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 89.9639 - mean_squared_error: 33473.6720 - val_loss: 86.8658 - val_mean_squared_error: 29310.2366\n",
            "Epoch 77/100\n",
            "4180/4180 [==============================] - 1s 240us/step - loss: 89.7117 - mean_squared_error: 30205.6061 - val_loss: 88.3112 - val_mean_squared_error: 28373.2204\n",
            "Epoch 78/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 89.2955 - mean_squared_error: 30791.4679 - val_loss: 86.2248 - val_mean_squared_error: 28075.4764\n",
            "Epoch 79/100\n",
            "4180/4180 [==============================] - 1s 227us/step - loss: 90.4405 - mean_squared_error: 30372.2427 - val_loss: 86.8344 - val_mean_squared_error: 28714.2108\n",
            "Epoch 80/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 88.6492 - mean_squared_error: 29552.8581 - val_loss: 85.1159 - val_mean_squared_error: 27650.2969\n",
            "Epoch 81/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 89.0348 - mean_squared_error: 30628.7702 - val_loss: 93.1372 - val_mean_squared_error: 33271.9508\n",
            "Epoch 82/100\n",
            "4180/4180 [==============================] - 1s 243us/step - loss: 88.7315 - mean_squared_error: 30096.2745 - val_loss: 89.1775 - val_mean_squared_error: 28749.7099\n",
            "Epoch 83/100\n",
            "4180/4180 [==============================] - 1s 239us/step - loss: 89.0117 - mean_squared_error: 30259.2539 - val_loss: 89.5444 - val_mean_squared_error: 28941.5637\n",
            "Epoch 84/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 88.4202 - mean_squared_error: 29558.0255 - val_loss: 87.8919 - val_mean_squared_error: 28415.5203\n",
            "Epoch 85/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 88.1245 - mean_squared_error: 29676.7026 - val_loss: 87.5439 - val_mean_squared_error: 28108.4906\n",
            "Epoch 86/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 87.9626 - mean_squared_error: 29669.9648 - val_loss: 85.7897 - val_mean_squared_error: 27224.8230\n",
            "Epoch 87/100\n",
            "4180/4180 [==============================] - 1s 244us/step - loss: 88.3286 - mean_squared_error: 29550.8739 - val_loss: 88.3756 - val_mean_squared_error: 28842.3826\n",
            "Epoch 88/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 91.6541 - mean_squared_error: 32530.0680 - val_loss: 88.2514 - val_mean_squared_error: 29050.9428\n",
            "Epoch 89/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 88.3035 - mean_squared_error: 30009.9903 - val_loss: 85.4867 - val_mean_squared_error: 28153.4463\n",
            "Epoch 90/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 87.3250 - mean_squared_error: 29508.6720 - val_loss: 86.5674 - val_mean_squared_error: 28401.6825\n",
            "Epoch 91/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 88.6937 - mean_squared_error: 29575.9400 - val_loss: 86.2396 - val_mean_squared_error: 28689.0451\n",
            "Epoch 92/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 88.6175 - mean_squared_error: 29680.4224 - val_loss: 87.6817 - val_mean_squared_error: 28475.6621\n",
            "Epoch 93/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 88.4173 - mean_squared_error: 30277.5698 - val_loss: 86.6699 - val_mean_squared_error: 27584.2612\n",
            "Epoch 94/100\n",
            "4180/4180 [==============================] - 1s 244us/step - loss: 86.8470 - mean_squared_error: 29092.4684 - val_loss: 87.7793 - val_mean_squared_error: 28843.6684\n",
            "Epoch 95/100\n",
            "4180/4180 [==============================] - 1s 232us/step - loss: 93.6711 - mean_squared_error: 118785.5331 - val_loss: 87.9629 - val_mean_squared_error: 28342.0369\n",
            "Epoch 96/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 88.7552 - mean_squared_error: 29500.2652 - val_loss: 88.4717 - val_mean_squared_error: 28131.5484\n",
            "Epoch 97/100\n",
            "4180/4180 [==============================] - 1s 237us/step - loss: 88.7273 - mean_squared_error: 29525.1702 - val_loss: 85.3912 - val_mean_squared_error: 27829.1141\n",
            "Epoch 98/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 89.5673 - mean_squared_error: 30367.0467 - val_loss: 87.4996 - val_mean_squared_error: 28972.2861\n",
            "Epoch 99/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 89.4105 - mean_squared_error: 29901.0839 - val_loss: 88.7624 - val_mean_squared_error: 28566.4277\n",
            "Epoch 100/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 87.9577 - mean_squared_error: 29384.6066 - val_loss: 88.8989 - val_mean_squared_error: 29065.7833\n",
            "31565.22012601402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished TreeExplainer\n",
            "len one 1\n",
            "len two 1\n",
            "len one 1\n",
            "len two 1\n",
            "final\n",
            "Feature shape: (237,)\n",
            "Train on 4180 samples, validate on 1046 samples\n",
            "Epoch 1/100\n",
            "4180/4180 [==============================] - 2s 367us/step - loss: 173275345974.8756 - mean_squared_error: 12857250968910950352027648.0000 - val_loss: 119510946826.7636 - val_mean_squared_error: 2710593754457381250531328.0000\n",
            "Epoch 2/100\n",
            "4180/4180 [==============================] - 1s 252us/step - loss: 41360044201.8635 - mean_squared_error: 4202390978520230884540416.0000 - val_loss: 159.7057 - val_mean_squared_error: 58685.1349\n",
            "Epoch 3/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 398.5300 - mean_squared_error: 236092895.9618 - val_loss: 156.4429 - val_mean_squared_error: 57386.8538\n",
            "Epoch 4/100\n",
            "4180/4180 [==============================] - 1s 259us/step - loss: 292080.1952 - mean_squared_error: 353322957003485.6250 - val_loss: 152.4581 - val_mean_squared_error: 55703.4737\n",
            "Epoch 5/100\n",
            "4180/4180 [==============================] - 1s 263us/step - loss: 154.6720 - mean_squared_error: 57728.7071 - val_loss: 149.9890 - val_mean_squared_error: 54600.8648\n",
            "Epoch 6/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 1939.8590 - mean_squared_error: 13134649282.2071 - val_loss: 147.9544 - val_mean_squared_error: 54020.4348\n",
            "Epoch 7/100\n",
            "4180/4180 [==============================] - 1s 258us/step - loss: 147.7362 - mean_squared_error: 54670.0313 - val_loss: 143.8571 - val_mean_squared_error: 52450.6641\n",
            "Epoch 8/100\n",
            "4180/4180 [==============================] - 1s 258us/step - loss: 146.7434 - mean_squared_error: 64974.7696 - val_loss: 140.7936 - val_mean_squared_error: 51430.2257\n",
            "Epoch 9/100\n",
            "4180/4180 [==============================] - 1s 258us/step - loss: 142.2370 - mean_squared_error: 52621.1443 - val_loss: 138.0732 - val_mean_squared_error: 50399.5505\n",
            "Epoch 10/100\n",
            "4180/4180 [==============================] - 1s 261us/step - loss: 139.4739 - mean_squared_error: 51707.8340 - val_loss: 135.6259 - val_mean_squared_error: 49589.3756\n",
            "Epoch 11/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 136.9444 - mean_squared_error: 50703.8416 - val_loss: 132.6564 - val_mean_squared_error: 48451.8987\n",
            "Epoch 12/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 134.2320 - mean_squared_error: 49826.5531 - val_loss: 130.0729 - val_mean_squared_error: 47467.1025\n",
            "Epoch 13/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 131.9540 - mean_squared_error: 49100.6007 - val_loss: 128.5842 - val_mean_squared_error: 47041.2894\n",
            "Epoch 14/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 129.0034 - mean_squared_error: 47963.9100 - val_loss: 125.9433 - val_mean_squared_error: 46009.1741\n",
            "Epoch 15/100\n",
            "4180/4180 [==============================] - 1s 268us/step - loss: 162342214.7319 - mean_squared_error: 106483027474480087040.0000 - val_loss: 12683618056.8065 - val_mean_squared_error: 30281674827097708691456.0000\n",
            "Epoch 16/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 404270416.1524 - mean_squared_error: 117841562475951013888.0000 - val_loss: 123.9302 - val_mean_squared_error: 52056.5232\n",
            "Epoch 17/100\n",
            "4180/4180 [==============================] - 1s 253us/step - loss: 123.4299 - mean_squared_error: 49873.5172 - val_loss: 121.9452 - val_mean_squared_error: 51175.1565\n",
            "Epoch 18/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 120.4503 - mean_squared_error: 44834.6322 - val_loss: 120.8122 - val_mean_squared_error: 50565.4844\n",
            "Epoch 19/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 118.5487 - mean_squared_error: 44193.3051 - val_loss: 118.4899 - val_mean_squared_error: 49826.1890\n",
            "Epoch 20/100\n",
            "4180/4180 [==============================] - 1s 264us/step - loss: 116.7001 - mean_squared_error: 43448.1394 - val_loss: 116.1412 - val_mean_squared_error: 49051.3060\n",
            "Epoch 21/100\n",
            "4180/4180 [==============================] - 1s 264us/step - loss: 115.0179 - mean_squared_error: 42648.0987 - val_loss: 114.4552 - val_mean_squared_error: 48343.8894\n",
            "Epoch 22/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 113.4450 - mean_squared_error: 42059.8506 - val_loss: 115.4269 - val_mean_squared_error: 48477.1033\n",
            "Epoch 23/100\n",
            "4180/4180 [==============================] - 1s 263us/step - loss: 112.7788 - mean_squared_error: 41786.0234 - val_loss: 112.5313 - val_mean_squared_error: 47450.4188\n",
            "Epoch 24/100\n",
            "4180/4180 [==============================] - 1s 248us/step - loss: 111.3190 - mean_squared_error: 41141.8066 - val_loss: 112.1762 - val_mean_squared_error: 47239.9241\n",
            "Epoch 25/100\n",
            "4180/4180 [==============================] - 1s 258us/step - loss: 792971.9097 - mean_squared_error: 2102031227369996.7500 - val_loss: 108.2347 - val_mean_squared_error: 39044.9720\n",
            "Epoch 26/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 109.1648 - mean_squared_error: 40060.9208 - val_loss: 106.5259 - val_mean_squared_error: 38268.0651\n",
            "Epoch 27/100\n",
            "4180/4180 [==============================] - 1s 271us/step - loss: 109.8308 - mean_squared_error: 46166.5973 - val_loss: 106.3345 - val_mean_squared_error: 38016.6947\n",
            "Epoch 28/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 107.6001 - mean_squared_error: 39108.4067 - val_loss: 105.5409 - val_mean_squared_error: 37635.9892\n",
            "Epoch 29/100\n",
            "4180/4180 [==============================] - 1s 276us/step - loss: 106.4283 - mean_squared_error: 38501.0188 - val_loss: 104.5667 - val_mean_squared_error: 36879.7473\n",
            "Epoch 30/100\n",
            "4180/4180 [==============================] - 1s 269us/step - loss: 286005621.8941 - mean_squared_error: 98245428229888901120.0000 - val_loss: 103.8740 - val_mean_squared_error: 36611.5462\n",
            "Epoch 31/100\n",
            "4180/4180 [==============================] - 1s 254us/step - loss: 105.3411 - mean_squared_error: 37955.2090 - val_loss: 104.0065 - val_mean_squared_error: 36731.1884\n",
            "Epoch 32/100\n",
            "4180/4180 [==============================] - 1s 254us/step - loss: 103.5991 - mean_squared_error: 37579.2386 - val_loss: 102.5971 - val_mean_squared_error: 36808.5756\n",
            "Epoch 33/100\n",
            "4180/4180 [==============================] - 1s 253us/step - loss: 102.0188 - mean_squared_error: 37223.2295 - val_loss: 99.6763 - val_mean_squared_error: 35405.5061\n",
            "Epoch 34/100\n",
            "4180/4180 [==============================] - 1s 252us/step - loss: 100.6561 - mean_squared_error: 36607.3181 - val_loss: 98.3154 - val_mean_squared_error: 35041.2003\n",
            "Epoch 35/100\n",
            "4180/4180 [==============================] - 1s 261us/step - loss: 99.8704 - mean_squared_error: 36198.7351 - val_loss: 97.0186 - val_mean_squared_error: 34458.0344\n",
            "Epoch 36/100\n",
            "4180/4180 [==============================] - 1s 269us/step - loss: 98.7889 - mean_squared_error: 35892.9228 - val_loss: 97.0854 - val_mean_squared_error: 34294.9516\n",
            "Epoch 37/100\n",
            "4180/4180 [==============================] - 1s 268us/step - loss: 97.7970 - mean_squared_error: 35461.0358 - val_loss: 97.6909 - val_mean_squared_error: 34192.7371\n",
            "Epoch 38/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 97.5255 - mean_squared_error: 35163.5768 - val_loss: 95.3933 - val_mean_squared_error: 33469.1386\n",
            "Epoch 39/100\n",
            "4180/4180 [==============================] - 1s 269us/step - loss: 97.1466 - mean_squared_error: 34965.2254 - val_loss: 95.4967 - val_mean_squared_error: 33390.8542\n",
            "Epoch 40/100\n",
            "4180/4180 [==============================] - 1s 261us/step - loss: 96.3721 - mean_squared_error: 34565.5733 - val_loss: 94.8913 - val_mean_squared_error: 33111.3887\n",
            "Epoch 41/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 95.9646 - mean_squared_error: 34285.6007 - val_loss: 95.1949 - val_mean_squared_error: 33060.1534\n",
            "Epoch 42/100\n",
            "4180/4180 [==============================] - 1s 256us/step - loss: 95.9978 - mean_squared_error: 34304.1655 - val_loss: 94.1028 - val_mean_squared_error: 32460.9262\n",
            "Epoch 43/100\n",
            "4180/4180 [==============================] - 1s 269us/step - loss: 95.1556 - mean_squared_error: 33866.7875 - val_loss: 94.3633 - val_mean_squared_error: 32815.2612\n",
            "Epoch 44/100\n",
            "4180/4180 [==============================] - 1s 249us/step - loss: 94.9525 - mean_squared_error: 33891.6779 - val_loss: 93.3602 - val_mean_squared_error: 32111.5417\n",
            "Epoch 45/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 94.5939 - mean_squared_error: 33413.8118 - val_loss: 93.8452 - val_mean_squared_error: 32270.9251\n",
            "Epoch 46/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 98.8209 - mean_squared_error: 58570.7638 - val_loss: 94.1826 - val_mean_squared_error: 32225.4622\n",
            "Epoch 47/100\n",
            "4180/4180 [==============================] - 1s 254us/step - loss: 96.5090 - mean_squared_error: 35490.0863 - val_loss: 94.1055 - val_mean_squared_error: 32005.3883\n",
            "Epoch 48/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 95.4452 - mean_squared_error: 33781.5754 - val_loss: 93.4820 - val_mean_squared_error: 31818.9284\n",
            "Epoch 49/100\n",
            "4180/4180 [==============================] - 1s 270us/step - loss: 95.0438 - mean_squared_error: 33578.1779 - val_loss: 93.5363 - val_mean_squared_error: 31879.8287\n",
            "Epoch 50/100\n",
            "4180/4180 [==============================] - 1s 289us/step - loss: 94.7908 - mean_squared_error: 33363.2014 - val_loss: 93.5694 - val_mean_squared_error: 31743.8402\n",
            "Epoch 51/100\n",
            "4180/4180 [==============================] - 1s 290us/step - loss: 94.4941 - mean_squared_error: 33247.2976 - val_loss: 93.7583 - val_mean_squared_error: 31579.5388\n",
            "Epoch 52/100\n",
            "4180/4180 [==============================] - 1s 288us/step - loss: 94.3476 - mean_squared_error: 33109.2047 - val_loss: 93.0345 - val_mean_squared_error: 31273.4995\n",
            "Epoch 53/100\n",
            "4180/4180 [==============================] - 1s 296us/step - loss: 94.4695 - mean_squared_error: 33035.2437 - val_loss: 92.7004 - val_mean_squared_error: 31283.3199\n",
            "Epoch 54/100\n",
            "4180/4180 [==============================] - 1s 270us/step - loss: 93.9405 - mean_squared_error: 32729.8861 - val_loss: 92.7935 - val_mean_squared_error: 30988.2305\n",
            "Epoch 55/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 93.7785 - mean_squared_error: 32641.9692 - val_loss: 92.5083 - val_mean_squared_error: 30879.1372\n",
            "Epoch 56/100\n",
            "4180/4180 [==============================] - 1s 258us/step - loss: 93.8036 - mean_squared_error: 32646.3411 - val_loss: 92.8294 - val_mean_squared_error: 30945.5907\n",
            "Epoch 57/100\n",
            "4180/4180 [==============================] - 1s 256us/step - loss: 93.7071 - mean_squared_error: 32518.3859 - val_loss: 93.1870 - val_mean_squared_error: 31141.2591\n",
            "Epoch 58/100\n",
            "4180/4180 [==============================] - 1s 266us/step - loss: 93.7286 - mean_squared_error: 32523.9454 - val_loss: 92.7726 - val_mean_squared_error: 30871.6676\n",
            "Epoch 59/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 93.6882 - mean_squared_error: 32453.1415 - val_loss: 92.9598 - val_mean_squared_error: 30851.5765\n",
            "Epoch 60/100\n",
            "4180/4180 [==============================] - 1s 247us/step - loss: 93.8888 - mean_squared_error: 32201.0884 - val_loss: 92.9573 - val_mean_squared_error: 30874.9488\n",
            "Epoch 61/100\n",
            "4180/4180 [==============================] - 1s 259us/step - loss: 93.6394 - mean_squared_error: 32335.2281 - val_loss: 92.4506 - val_mean_squared_error: 30620.1341\n",
            "Epoch 62/100\n",
            "4180/4180 [==============================] - 1s 263us/step - loss: 93.5329 - mean_squared_error: 32033.9332 - val_loss: 92.5737 - val_mean_squared_error: 30680.2323\n",
            "Epoch 63/100\n",
            "4180/4180 [==============================] - 1s 256us/step - loss: 93.4831 - mean_squared_error: 32140.2928 - val_loss: 92.5715 - val_mean_squared_error: 30738.7329\n",
            "Epoch 64/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 93.6652 - mean_squared_error: 32235.2720 - val_loss: 93.2633 - val_mean_squared_error: 30888.5089\n",
            "Epoch 65/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 93.8116 - mean_squared_error: 32117.1668 - val_loss: 92.7883 - val_mean_squared_error: 30629.1765\n",
            "Epoch 66/100\n",
            "4180/4180 [==============================] - 1s 253us/step - loss: 93.5309 - mean_squared_error: 32058.5116 - val_loss: 92.8102 - val_mean_squared_error: 30694.4668\n",
            "Epoch 67/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 93.2969 - mean_squared_error: 31970.2063 - val_loss: 92.8374 - val_mean_squared_error: 30587.3973\n",
            "Epoch 68/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 93.6203 - mean_squared_error: 32157.8479 - val_loss: 92.9643 - val_mean_squared_error: 30887.5535\n",
            "Epoch 69/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 93.2991 - mean_squared_error: 31954.9893 - val_loss: 92.5449 - val_mean_squared_error: 30486.2657\n",
            "Epoch 70/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 93.4002 - mean_squared_error: 32083.1954 - val_loss: 93.1500 - val_mean_squared_error: 30733.4353\n",
            "Epoch 71/100\n",
            "4180/4180 [==============================] - 1s 254us/step - loss: 93.5183 - mean_squared_error: 32013.6155 - val_loss: 92.9830 - val_mean_squared_error: 30560.3358\n",
            "Epoch 72/100\n",
            "4180/4180 [==============================] - 1s 252us/step - loss: 93.2425 - mean_squared_error: 31801.5016 - val_loss: 92.7120 - val_mean_squared_error: 30492.9744\n",
            "Epoch 73/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 93.5481 - mean_squared_error: 31919.2178 - val_loss: 93.2511 - val_mean_squared_error: 30770.5022\n",
            "Epoch 74/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 93.2285 - mean_squared_error: 31790.3134 - val_loss: 93.0392 - val_mean_squared_error: 30544.0668\n",
            "Epoch 75/100\n",
            "4180/4180 [==============================] - 1s 261us/step - loss: 93.0556 - mean_squared_error: 31577.8180 - val_loss: 92.9793 - val_mean_squared_error: 30604.9822\n",
            "Epoch 76/100\n",
            "4180/4180 [==============================] - 1s 257us/step - loss: 89043.5547 - mean_squared_error: 32919616933454.0273 - val_loss: 103.0247 - val_mean_squared_error: 31403.5781\n",
            "Epoch 77/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 19017046359.1520 - mean_squared_error: 1052196461399532294373376.0000 - val_loss: 98.3426 - val_mean_squared_error: 36220.9007\n",
            "Epoch 78/100\n",
            "4180/4180 [==============================] - 1s 261us/step - loss: 16730.6016 - mean_squared_error: 1086860274495.8556 - val_loss: 92.5424 - val_mean_squared_error: 30399.9837\n",
            "Epoch 79/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 93.9957 - mean_squared_error: 32097.9235 - val_loss: 92.7962 - val_mean_squared_error: 30779.3118\n",
            "Epoch 80/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 93.7868 - mean_squared_error: 32092.4205 - val_loss: 92.4470 - val_mean_squared_error: 30411.2137\n",
            "Epoch 81/100\n",
            "4180/4180 [==============================] - 1s 253us/step - loss: 93.6324 - mean_squared_error: 31869.2777 - val_loss: 93.9390 - val_mean_squared_error: 30912.7918\n",
            "Epoch 82/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 93.8074 - mean_squared_error: 32117.5353 - val_loss: 92.0786 - val_mean_squared_error: 30083.3945\n",
            "Epoch 83/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 94.1402 - mean_squared_error: 32139.6586 - val_loss: 93.6674 - val_mean_squared_error: 30446.4954\n",
            "Epoch 84/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 93.6760 - mean_squared_error: 31799.9502 - val_loss: 92.5387 - val_mean_squared_error: 30378.7310\n",
            "Epoch 85/100\n",
            "4180/4180 [==============================] - 1s 256us/step - loss: 93.9543 - mean_squared_error: 31882.4841 - val_loss: 92.6085 - val_mean_squared_error: 30164.8406\n",
            "Epoch 86/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 93.6139 - mean_squared_error: 31857.0465 - val_loss: 24781.0388 - val_mean_squared_error: 350496321440.5301\n",
            "Epoch 87/100\n",
            "4180/4180 [==============================] - 1s 262us/step - loss: 94.1657 - mean_squared_error: 31945.7926 - val_loss: 94.6311 - val_mean_squared_error: 30584.7667\n",
            "Epoch 88/100\n",
            "4180/4180 [==============================] - 1s 259us/step - loss: 3953.6742 - mean_squared_error: 48257079638.2438 - val_loss: 92.5074 - val_mean_squared_error: 30201.2380\n",
            "Epoch 89/100\n",
            "4180/4180 [==============================] - 1s 259us/step - loss: 93.8112 - mean_squared_error: 31815.9298 - val_loss: 92.7142 - val_mean_squared_error: 30286.6452\n",
            "Epoch 90/100\n",
            "4180/4180 [==============================] - 1s 256us/step - loss: 93.8057 - mean_squared_error: 31698.2991 - val_loss: 93.5166 - val_mean_squared_error: 30307.3777\n",
            "Epoch 91/100\n",
            "4180/4180 [==============================] - 1s 263us/step - loss: 94.2362 - mean_squared_error: 31994.5177 - val_loss: 93.7011 - val_mean_squared_error: 30101.0234\n",
            "Epoch 92/100\n",
            "4180/4180 [==============================] - 1s 266us/step - loss: 94.5741 - mean_squared_error: 32202.8985 - val_loss: 93.6615 - val_mean_squared_error: 30943.0847\n",
            "Epoch 93/100\n",
            "4180/4180 [==============================] - 1s 284us/step - loss: 94.2185 - mean_squared_error: 31950.7669 - val_loss: 92.6738 - val_mean_squared_error: 30688.4464\n",
            "Epoch 94/100\n",
            "4180/4180 [==============================] - 1s 285us/step - loss: 93.9671 - mean_squared_error: 32008.6201 - val_loss: 93.0311 - val_mean_squared_error: 30585.5625\n",
            "Epoch 95/100\n",
            "4180/4180 [==============================] - 1s 265us/step - loss: 94.0686 - mean_squared_error: 31819.4831 - val_loss: 92.8061 - val_mean_squared_error: 30289.9046\n",
            "Epoch 96/100\n",
            "4180/4180 [==============================] - 1s 272us/step - loss: 95.1068 - mean_squared_error: 33002.8400 - val_loss: 92.9997 - val_mean_squared_error: 30348.9984\n",
            "Epoch 97/100\n",
            "4180/4180 [==============================] - 1s 276us/step - loss: 94.0977 - mean_squared_error: 31801.3849 - val_loss: 93.0696 - val_mean_squared_error: 30486.9494\n",
            "Epoch 98/100\n",
            "4180/4180 [==============================] - 1s 270us/step - loss: 453.2815 - mean_squared_error: 539796106.6249 - val_loss: 92.5059 - val_mean_squared_error: 30100.3998\n",
            "Epoch 99/100\n",
            "4180/4180 [==============================] - 1s 273us/step - loss: 94.5385 - mean_squared_error: 31870.2573 - val_loss: 93.6885 - val_mean_squared_error: 30234.6438\n",
            "Epoch 100/100\n",
            "4180/4180 [==============================] - 1s 272us/step - loss: 93.9719 - mean_squared_error: 31750.8264 - val_loss: 92.4587 - val_mean_squared_error: 30373.4041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rMAYjqbV8SS",
        "colab_type": "code",
        "outputId": "aa253a98-805f-490c-d2b5-37a700624960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "new_first_tree, new_second_tree, shapper_tree, shap_select_abs_tree = runner(first, second, target, modeller=\"LightGBM\")"
      ],
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished TreeExplainer\n",
            "11639.838583703673\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished TreeExplainer\n",
            "len one 22\n",
            "len two 22\n",
            "len one 22\n",
            "len two 22\n",
            "final\n",
            "Finished TreeExplainer\n",
            "11997.976374663187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished TreeExplainer\n",
            "len one 20\n",
            "len two 20\n",
            "len one 20\n",
            "len two 20\n",
            "final\n",
            "Finished TreeExplainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeDAKg5G_7oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def all(new_first_two,new_second_two, shapper,target, dall=False ):\n",
        "  def scaler(df):\n",
        "    x = df.values #returns a numpy array\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled, index=df.index, columns=df.columns)\n",
        "    return df\n",
        "\n",
        "  if dall:\n",
        "    shapper = new_first_two.columns\n",
        "  \n",
        "\n",
        "  new_first_y = new_first_two[target].copy()\n",
        "  new_first = new_first_two[shapper].copy()\n",
        "  new_first = scaler(new_first.drop([target],axis=1)).copy()\n",
        "\n",
        "  new_second_y = new_second_two[target].copy()\n",
        "  new_second = new_second_two[shapper].copy()\n",
        "  new_second = scaler(new_second.drop([target],axis=1)).copy() \n",
        "\n",
        "  return new_first,new_first_y, new_second, new_second_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xWS_ASQeHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_deep_one,new_first_y, new_deep_two, new_second_y = all(new_first_deep,new_second_deep, shapper_deep,target , True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFDv1FvBY38J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_first = new_deep_one.copy()\n",
        "new_second = new_deep_two.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOT0UpGcOOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tree\n",
        "new_tree_one,_, new_tree_two,_  = all(new_first_tree,new_second_tree, shapper_tree,target, True)\n",
        "new_first = new_tree_one.copy()\n",
        "new_second = new_tree_two.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T5tgnXOZq4M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "54fd8c0a-fba4-4a6d-8cd3-bae0b4fe2c5a"
      },
      "source": [
        "lass = LassoLarsCV(cv=5).fit(new_first, new_first_y)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressors in active set degenerate. Dropping a regressor, after 339 iterations, i.e. alpha=1.105e-04, with an active set of 143 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 340 iterations, alpha=1.091e-04, previous alpha=1.067e-04, with an active set of 143 regressors.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 469 iterations, i.e. alpha=1.663e-05, with an active set of 171 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 499 iterations, i.e. alpha=1.173e-05, with an active set of 171 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 499 iterations, i.e. alpha=1.145e-05, with an active set of 171 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 365 iterations, i.e. alpha=9.164e-05, with an active set of 147 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 367 iterations, alpha=9.092e-05, previous alpha=8.990e-05, with an active set of 148 regressors.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 416 iterations, i.e. alpha=3.468e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 426 iterations, i.e. alpha=3.077e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 426 iterations, i.e. alpha=3.074e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 431 iterations, alpha=2.663e-05, previous alpha=2.640e-05, with an active set of 164 regressors.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 310 iterations, i.e. alpha=1.766e-04, with an active set of 148 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "Regressors in active set degenerate. Dropping a regressor, after 357 iterations, i.e. alpha=5.816e-05, with an active set of 151 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 364 iterations, alpha=5.538e-05, previous alpha=5.505e-05, with an active set of 157 regressors.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZogoQlAEJL9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b0c38a20-0080-46a8-9880-849dfe141914"
      },
      "source": [
        "lass"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LassoLarsCV(copy_X=True, cv=5, eps=2.220446049250313e-16, fit_intercept=True,\n",
              "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
              "            positive=False, precompute='auto', verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPGbL75SiNwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba06312b-ce85-4cf5-e8ab-406629bca0c5"
      },
      "source": [
        "## Tree\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16241.6754428426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMcsCcIM0n7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6147f3cc-bc01-4442-f6d4-3c2e3224c1bf"
      },
      "source": [
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15129.27870045936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNElFfQpVbZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mse = pd.DataFrame(columns=[\"alpha\",\"mse\"])\n",
        "# mse[\"alpha\"]= [ra.sum() for ra in lass.alphas_]\n",
        "# mse[\"mse\"] = [ra.sum() for ra in lass.mse_path_]\n",
        "# mse.sort_values(\"mse\").to_csv(\"dd.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmM896HZh1lD",
        "colab_type": "code",
        "outputId": "839bdd30-4b8d-4250-8d41-a394cecefc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Lasso(alpha=0.1, tol =0.1)\n",
        "lass = clf.fit(new_first,new_first_y)\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "# #12381 0.10\n"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13448.410623256994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KWWCkXAxHa--",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "# max_features\n",
        "\n",
        "#new_first = new_deep_one.copy()\n",
        "#new_second = new_deep_two.copy()\n",
        "\n",
        "model = SelectFromModel(lass, prefit=True)\n",
        "X_train = new_first[new_first.columns[model.get_support(indices=True)].copy()].copy()\n",
        "#new_first_y = new_first_two[target]\n",
        "X_test = new_second[list(X_train.columns)].copy()\n",
        "#new_second_y = new_second_two[target]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6e678453-1efc-4d56-8b2e-910af9c9d935",
        "id": "UfCZIwiMHapA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train.loc[:,~X_train.columns.duplicated()].shape"
      ],
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5226, 84)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NbyJyCSRnbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "064a98d5-92d6-47d5-d3c1-b3edb96c48d5"
      },
      "source": [
        "X_test.columns"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['review_scores_rating', 'number_of_reviews', 'minimum_nights',\n",
              "       'security_deposit', 'beds', 'availability_365', 'bedrooms_per_person',\n",
              "       'Bondi', 'Coogee', 'North Bondi', 'Randwick', 'Redfern', 'Sydney',\n",
              "       'Boutique hotel', 'Bungalow', 'Condominium', 'Cottage', 'Guest suite',\n",
              "       'Guesthouse', 'Hostel', 'Hotel', 'House', 'Other', 'Serviced apartment',\n",
              "       'Townhouse', 'Villa', 'Hotel room', 'Shared room', 'luxury_moderate',\n",
              "       'moderate', 'strict_14_with_grace_period', 'super_strict_30',\n",
              "       'super_strict_60', 'security_deposit_X_cleaning_fee',\n",
              "       'bedrooms_X_cleaning_fee', 'bedrooms_X_beds',\n",
              "       'bathrooms_X_security_deposit',\n",
              "       'cleaning_fee_X_past_and_future_popularity',\n",
              "       'Entire home/apt_X_bathrooms', 'Entire home/apt_X_cleaning_fee',\n",
              "       'accommodates_X_Private room', 'bathrooms_DIV_bedrooms',\n",
              "       'security_deposit_DIV_cleaning_fee', 'bedrooms_DIV_beds',\n",
              "       'bathrooms_DIV_security_deposit', 'Entire home/apt_DIV_bathrooms',\n",
              "       'security_deposit_POWER_2', 'bathrooms_per_person_POWER_2',\n",
              "       'beds_POWER_2', 'cleaning_fee_LOG_p_one_abs', 'beds_RECIP_p_one',\n",
              "       'past_and_future_popularity_RECIP_p_one',\n",
              "       'bathrooms_X_bedrooms_X_longitude',\n",
              "       'accommodates_DIV_past_and_future_popularity_X_bathrooms_per_person_DIV_cleaning_fee',\n",
              "       'accommodates_X_bedrooms_X_bathrooms_per_person_DIV_cleaning_fee',\n",
              "       'accommodates_X_security_deposit_X_bathrooms_per_person_DIV_cleaning_fee',\n",
              "       'accommodates_X_security_deposit_X_longitude',\n",
              "       'bathrooms_X_bedrooms_DIV_accommodates_X_security_deposit',\n",
              "       'bathrooms_X_bedrooms_DIV_accommodates_DIV_past_and_future_popularity',\n",
              "       'accommodates_X_bedrooms_DIV_accommodates_DIV_past_and_future_popularity',\n",
              "       'bathrooms_DIV_cleaning_fee_DIV_bathrooms_per_person_DIV_cleaning_fee',\n",
              "       'accommodates_X_bedrooms_DIV_bathrooms_X_bedrooms',\n",
              "       'accommodates_X_bedrooms_DIV_Entire home/apt_DIV_bathrooms_per_person',\n",
              "       'accommodates_X_bedrooms_POWER_2', 'bathrooms_X_bedrooms_POWER_2',\n",
              "       'Entire home/apt_DIV_bathrooms_per_person_POWER_2',\n",
              "       'Entire home/apt_X_bedrooms_POWER_2',\n",
              "       'bathrooms_per_person_DIV_cleaning_fee_POWER_2',\n",
              "       'accommodates_X_cleaning_fee_POWER_2', 'longitude_POWER_2',\n",
              "       'latitude_POWER_2',\n",
              "       'accommodates_X_past_and_future_popularity_RECIP_p_one'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VrwHL3Vg_c5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "52b8974d-d606-48ea-82bd-7a9907706873"
      },
      "source": [
        "## Deep\n",
        "\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train,new_first_y)\n",
        "preds = lm.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13756.481939623793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD47uuK5bqUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11093b77-2ed6-496b-e421-fba11579e22a"
      },
      "source": [
        "## Tree\n",
        "\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train,new_first_y)\n",
        "preds = lm.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15003.467159303042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv269iEyl57v",
        "colab_type": "code",
        "outputId": "f8de5661-fe3a-4801-ebe4-55c0c39c7c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "## Tree\n",
        "\n",
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train,new_first_y)\n",
        "preds = lm.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13163.966378637819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJIzTP-_PH4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "748ed5b9-6c10-4402-b949-cb6850ac5c2e"
      },
      "source": [
        "preds"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 19.24207033, 381.4889417 , 315.06818663, ..., 204.10551279,\n",
              "       118.35484246, 106.50990425])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdtY_TLvB_45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def scaler(df):\n",
        "  x = df.values #returns a numpy array\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df = pd.DataFrame(x_scaled)\n",
        "  return df\n",
        "\n",
        "add_first_y = first[target]\n",
        "add_first = scaler(first.drop([target],axis=1))\n",
        "\n",
        "add_second_y = second[target]\n",
        "add_second = scaler(second.drop([target],axis=1)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06e-K1qFDYwC",
        "colab_type": "code",
        "outputId": "dac45490-ce5e-4bce-f53f-3e44b2cffb46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "#clf = linear_model.Lasso(alpha=0.4)\n",
        "clf = linear_model.LinearRegression()\n",
        "preds = clf.fit(add_first,add_first_y).predict(add_second)\n",
        "mse = mean_squared_error(add_second_y, preds)\n",
        "print(mse)\n"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14287.356235987374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiaJWrrcRJq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "16d94270-730b-4283-dff3-e4d8443d3812"
      },
      "source": [
        "from sklearn import linear_model\n",
        "#clf = linear_model.Lasso(alpha=0.4)\n",
        "clf = linear_model.LinearRegression()\n",
        "preds = clf.fit(add_first,add_first_y).predict(add_second)\n",
        "mse = mean_squared_error(add_second_y, preds)\n",
        "print(mse)\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-28f684910d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clf = linear_model.Lasso(alpha=0.4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_first\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_first_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_second\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_second_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"raise_missing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_setter\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         )\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 raise KeyError(\n\u001b[1;32m   1176\u001b[0m                     \"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1177\u001b[0;31m                         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m                     )\n\u001b[1;32m   1179\u001b[0m                 )\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['longitude', 'latitude', 'number_of_reviews', 'security_deposit',\\n       'cleaning_fee', 'accommodates', 'bathrooms', 'bedrooms', 'beds',\\n       'host_identity_verified',\\n       ...\\n       'Shared room_RECIP_p_one', 'Surry Hills_RECIP_p_one',\\n       'Private room_SQRT_p_one', 'availability_365_SQRT_p_one',\\n       'Entire home/apt_SQRT_p_one', 'bedrooms_SQRT_p_one',\\n       'accommodates_SQRT_p_one', 'security_deposit_SQRT_p_one',\\n       'Manly_SQRT_p_one', 'Surry Hills_SQRT_p_one'],\\n      dtype='object', length=135)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWnWZjN88uSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# poly_reg = PolynomialFeatures(degree=3)\n",
        "# PR = poly_reg.fit(add_first)\n",
        "\n",
        "# X_poly = PR.transform(add_first)\n",
        "# X_pred = PR.transform(add_second)\n",
        "\n",
        "# pol_reg = linear_model.LinearRegression()\n",
        "# pol_reg.fit(X_poly, add_first_y)\n",
        "# preds = pol_reg.predict(X_pred)\n",
        "# mse = mean_squared_error(add_second_y, preds)\n",
        "# print(mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtixIRs6aucm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# shapper_tree = shapper_tree[:10]\n",
        "# shapper_tree.append(target)\n",
        "\n",
        "new_tree_one,_, new_tree_two,_  = all(new_first_tree,new_second_tree, shapper_tree,target, True)\n",
        "new_first = new_tree_one.copy()\n",
        "new_second = new_tree_two.copy()\n",
        "\n",
        "\n",
        "# ###  comes here\n",
        "\n",
        "\n",
        "# new_first =  pd.concat((new_tree_one,new_deep_one ),axis=1)\n",
        "# new_second =  pd.concat((new_tree_two,new_deep_two ),axis=1)\n",
        "\n",
        "# new_first = new_first.loc[:,~new_first.columns.duplicated()].copy()\n",
        "# new_second = new_second.loc[:,~new_second.columns.duplicated()].copy()\n",
        "\n",
        "# ### comes here\n",
        "\n",
        "# new_first = new_tree_one.copy()\n",
        "# new_second = new_tree_two.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}