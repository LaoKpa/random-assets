{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firmai/random-assets/blob/master/Linear_Data2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-5xTpxclAKT",
        "colab_type": "code",
        "outputId": "6dbdd200-1e22-4e32-ac06-c2772664e37b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/e2/4050c2e68639adf0f8a8a4857f234b71fe1a1139e25ff17575c935f49615/shap-0.33.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 204kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (0.25.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.28.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.14.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.33.0-cp36-cp36m-linux_x86_64.whl size=382254 sha256=dcf8a6effe0df11220e13f486d3319db1a8fc63fdc93d9d9010059ff4f3007d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/0f/88/a8124d43431284e10f263ffe449e119344c6145c3a165d186c\n",
            "Successfully built shap\n",
            "Installing collected packages: shap\n",
            "Successfully installed shap-0.33.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9zqd1BV5qPB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67824bab-b44b-445d-cfe9-7089678bda42"
      },
      "source": [
        "import pandas as pd \n",
        "import shap\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "X_pr = pd.read_csv(\"https://github.com/firmai/random-assets/blob/master/data_clean.csv?raw=true\").set_index(\"Unnamed: 0\", drop=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy9GLflalIsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pr.to_csv(\"frame.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5CD-1NufeKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_pr = pd.read_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XNRijX-h8z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNqEBTeskiI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_pr.index.name = 'index'\n",
        "target = \"price\"\n",
        "first = X_pr.sample(int(len(X_pr)/2))\n",
        "second = X_pr[~X_pr.isin(first)].dropna()\n",
        "\n",
        "\n",
        "def modelling(first, second, target, inter_portion=0.5, sqr_portion=0.5, contribution_portion=0.9, final_contribution=0.9, final=False, modeller=\"LightGBM\"):\n",
        "  \n",
        "  cols_drop = [target ]\n",
        "\n",
        "  def shap_frame(first,second, target):\n",
        "\n",
        "    #could also use linear booster XGBoost\n",
        "    d_train = lgb.Dataset(first.drop(columns=[target]), label=first[target])\n",
        "    d_valid = lgb.Dataset(second.drop(columns=[target]), label=second[target])\n",
        "    params = {\n",
        "      \n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmsle',\n",
        "        'max_depth': 6, \n",
        "        'learning_rate': 0.1,\n",
        "        'verbose': 0,\n",
        "      'num_threads':16}\n",
        "    n_estimators = 100\n",
        "\n",
        "    model = lgb.train(params, d_train, 100, verbose_eval=1)\n",
        "\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(first.drop([target], axis=1))\n",
        "    shap_fram = pd.DataFrame(shap_values[:,:], columns=list(first.drop([target], axis=1).columns))\n",
        "    shap_new = shap_fram.sum().sort_values().to_frame()\n",
        "    print(\"Finished TreeExplainer\")\n",
        "    \n",
        "    return shap_new, explainer, model\n",
        "\n",
        "  def shap_frame_keras(first,second, target):\n",
        "    ## deepexplainer and gradientexplainer has no interaction_values\n",
        "\n",
        "    # Set the input shape\n",
        "    input_shape = (len(first.columns)-1,)\n",
        "    print(f'Feature shape: {input_shape}')\n",
        "\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(int(4/(contribution_portion/8)), input_shape=input_shape, activation='relu'))\n",
        "    model.add(Dense(int(2/(contribution_portion/8)), activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "    # Configure the model and start training\n",
        "    # validation_data=(X_test, Y_test)\n",
        "\n",
        "\n",
        "    model.compile(loss='mean_absolute_error', optimizer=Adam(lr=0.01), metrics=['mean_squared_error'])\n",
        "\n",
        "    stoppy = EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "    model.fit(first.drop(columns=[target]), first[target], epochs=100, batch_size=10, verbose=1, validation_split=0.2, callbacks=[stoppy])\n",
        "\n",
        "    #explainer = shap.DeepExplainer(model,first.drop(columns=[target]).values)\n",
        "\n",
        "    explainer  = shap.GradientExplainer(model,first.drop(columns=[target]).values)\n",
        "\n",
        "    shap_values = explainer.shap_values(first.drop([target], axis=1).values)\n",
        "    shap_fram = pd.DataFrame(shap_values[0][:,:], columns=list(first.drop([target], axis=1).columns))\n",
        "    shap_new = shap_fram.sum().sort_values().to_frame()\n",
        "\n",
        "    return shap_new, explainer, model\n",
        "\n",
        "  if modeller==\"Keras\":\n",
        "    shape_frame_all = shap_frame_keras\n",
        "  else:\n",
        "    shape_frame_all = shap_frame\n",
        "\n",
        "##new_df = first.drop(cols_drop, axis=1)\n",
        "\n",
        "### Main Features\n",
        "\n",
        "### Main Calculations\n",
        "##shap_new, explainer, model = shap_frame(first,second,target)\n",
        "\n",
        "#shap_new, explainer, model = shap_frame_keras(first,second,target)\n",
        "\n",
        "\n",
        "  ##new_df = first.drop(cols_drop, axis=1)\n",
        "\n",
        "  ### Main Features\n",
        "\n",
        "  ### Main Calculations\n",
        "\n",
        "  shap_new, explainer, model = shape_frame_all(first,second,target)\n",
        "\n",
        "  shap_new_abs = shap_new[0].abs()\n",
        "  shap_new_abs = shap_new_abs.sort_values(ascending=False)\n",
        "  main_ft = shap_new_abs[shap_new_abs.cumsum().sub((shap_new_abs.sum()*sqr_portion)).le(0)]\n",
        "\n",
        "  preds = model.predict(second.drop(columns=[target]))\n",
        "  mse = mean_squared_error(second[target], preds)\n",
        "  print(mse)\n",
        "\n",
        "  def main_calc(new_df,main_ft):\n",
        "    df_square = new_df[list(main_ft.index)]\n",
        "    sqr_name = [fa+\"_POWER_2\" for fa in df_square.columns]\n",
        "    log_p_name = [fa+\"_LOG_p_one_abs\" for fa in df_square.columns]\n",
        "    rec_p_name = [fa+\"_RECIP_p_one\" for fa in df_square.columns]\n",
        "    sqrt_name = [fa+\"_SQRT_p_one\" for fa in df_square.columns]\n",
        "\n",
        "    df_sqr = pd.DataFrame(np.power(df_square.values, 2),columns=sqr_name, index=new_df.index)\n",
        "    df_log = pd.DataFrame(np.log(df_square.add(1).abs().values),columns=log_p_name, index=new_df.index)\n",
        "    df_rec = pd.DataFrame(np.reciprocal(df_square.add(1).values),columns=rec_p_name, index=new_df.index)\n",
        "    df_sqrt = pd.DataFrame(np.sqrt(df_square.abs().add(1).values),columns=sqrt_name, index=new_df.index)\n",
        "\n",
        "    dfs = [df_sqr, df_log, df_rec, df_sqrt]\n",
        "\n",
        "    df_connect=  pd.concat(dfs, axis=1)\n",
        "\n",
        "    return df_connect\n",
        "\n",
        "  ## An attempt to decrease the amount of interactin features.\n",
        "\n",
        "  select_ft = shap_new_abs[shap_new_abs.cumsum().sub((shap_new_abs.sum()*contribution_portion)).le(0)]\n",
        "  select_ft = list(select_ft.index)\n",
        "  select_ft.append(target)\n",
        "\n",
        "  ## has to remain tree for interaction effects. shap_frame\n",
        "  shap_select, explainer, model = shap_frame(first[select_ft], second[select_ft], target )\n",
        "  shap_select_abs = shap_select[0].abs()\n",
        "\n",
        "  ### Interactions Features\n",
        "  shap_interaction_values = explainer.shap_interaction_values(first[select_ft].drop(cols_drop, axis=1))\n",
        "\n",
        "  shap_interaction_values_abs = abs(shap_interaction_values)\n",
        "  df_start = pd.DataFrame(np.sum(shap_interaction_values_abs ,axis=0),columns=first[select_ft].drop(cols_drop, axis=1).columns, index=first[select_ft].drop(cols_drop, axis=1).columns)\n",
        "\n",
        "  #the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
        "  sol = (df_start.where(np.triu(np.ones(df_start.shape), k=1).astype(np.bool))\n",
        "                  .stack()\n",
        "                  .sort_values(ascending=False))\n",
        "  #first element of sol series is the pair with the bigest correlation\n",
        "\n",
        "  ## New Data Frames From Feature Interaction and Main Feature\n",
        "  ## If you have features [a, b, c] the default polynomial features(in sklearn the degree is 2) should be [1, a, b, c, a^2, b^2, c^2, ab, bc, ca].\n",
        "\n",
        "  ## Interaction Calculations\n",
        "\n",
        "  dab = sol[sol.cumsum().sub((sol.sum()*inter_portion)).le(0)]\n",
        "\n",
        "  list_one = [da[0] for da in dab.index]\n",
        "  list_two = [da[1] for da in dab.index]\n",
        "\n",
        "  def inter_cal(list_one, list_two,new_df):\n",
        "\n",
        "    mult = [ra+\"_X_\"+ba for ra, ba in zip(list_one, list_two)]\n",
        "    div = [ra+\"_DIV_\"+ba for ra, ba in zip(list_one, list_two)]\n",
        "    print(\"len one \" + str(len(list_one)) )\n",
        "    print(\"len two \" + str(len(list_two)) )\n",
        "    inter_mult = pd.DataFrame(new_df[list_one].values*new_df[list_two].values, columns=mult, index=new_df.index)\n",
        "    div_p_one = pd.DataFrame(new_df[list_one].add(1).values/new_df[list_two].add(1).values, columns=div, index=new_df.index)\n",
        "\n",
        "    df_one = pd.concat((inter_mult,div_p_one), axis=1)\n",
        "\n",
        "    return df_one\n",
        "\n",
        "\n",
        "  def combine(target, list_one, list_two, main_ft, new_df):\n",
        "\n",
        "    inter_mult = inter_cal(list_one, list_two, new_df.drop(columns=[target]))\n",
        "\n",
        "    df_sqr = main_calc(new_df.drop(columns=[target]),main_ft)\n",
        "\n",
        "    new = pd.concat((inter_mult,df_sqr),axis=1)\n",
        "    new2 = pd.concat((new_df,new),axis=1)\n",
        "\n",
        "    new2 = new2.loc[:,~new2.columns.duplicated()]\n",
        "\n",
        "    return new2\n",
        "\n",
        "  new_first = combine(target, list_one, list_two, main_ft, first)\n",
        "\n",
        "  new_second = combine(target, list_one, list_two, main_ft, second)\n",
        "\n",
        "  if final:\n",
        "    print(\"final\")\n",
        "    shap_select, explainer, model = shape_frame_all(new_first, new_second,target )\n",
        "    ### Can be put into function, somewhat unnecessary\n",
        "    shap_select_abs = shap_select[0].abs()\n",
        "    shap_select_abs = shap_select_abs.sort_values(ascending=False)\n",
        "    final_ft = shap_select_abs[shap_select_abs.cumsum().sub((shap_select_abs.sum()*final_contribution)).le(0)]\n",
        "    final_ft = list(final_ft.index)\n",
        "    final_ft.append(target)\n",
        "    return new_first,new_second, mse,final_ft, shap_select_abs\n",
        "\n",
        "  return new_first,new_second, mse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT21QuoEHJDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "## Definitely a benefit to run twice but not three times. \n",
        "def runner(first_run, second_run, target, inter_portion=0.8, sqr_portion=0.9, contribution_portion=0.9, final_contribution=0.95, deflator=0.7, runs=1, modeller=\"LightGBM\"):\n",
        "#def runner(first_run, second_run, target, inter_portion=0.4, sqr_portion=0.3, contribution_portion=0.9, deflator=0.6, runs=3):\n",
        "  for r in range(runs):\n",
        "    r += 1\n",
        "    if r ==runs:\n",
        "      print(\"final\")\n",
        "      first_run, second_run, mse, shapper, shap_select_abs = modelling(first_run, second_run, target, inter_portion, sqr_portion, contribution_portion, final_contribution, True, modeller)\n",
        "    else:\n",
        "      first_run, second_run, mse = modelling(first_run, second_run, target, inter_portion, sqr_portion, contribution_portion, final_contribution, False, modeller)\n",
        "\n",
        "    inter_portion = inter_portion * deflator\n",
        "    sqr_portion = sqr_portion * deflator * 1.1\n",
        "    contribution_portion = contribution_portion * deflator\n",
        "    gc.collect()\n",
        "  \n",
        "  return first_run, second_run, shapper, shap_select_abs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SArcKbhwh0h9",
        "colab_type": "code",
        "outputId": "c85030bc-7771-49b4-e11d-aa3f060382ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "new_first_deep, new_second_deep, shapper_deep, shap_select_abs = runner(first, second, target, modeller=\"Keras\")"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final\n",
            "Feature shape: (53,)\n",
            "Train on 4180 samples, validate on 1046 samples\n",
            "Epoch 1/100\n",
            "4180/4180 [==============================] - 4s 963us/step - loss: 83.0470 - mean_squared_error: 22007.1792 - val_loss: 80.2617 - val_mean_squared_error: 20489.5200\n",
            "Epoch 2/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 72.1941 - mean_squared_error: 17831.1802 - val_loss: 78.0598 - val_mean_squared_error: 17306.5661\n",
            "Epoch 3/100\n",
            "4180/4180 [==============================] - 1s 246us/step - loss: 67.6649 - mean_squared_error: 15931.4927 - val_loss: 76.5550 - val_mean_squared_error: 16036.9465\n",
            "Epoch 4/100\n",
            "4180/4180 [==============================] - 1s 227us/step - loss: 69.1321 - mean_squared_error: 16276.6211 - val_loss: 71.7900 - val_mean_squared_error: 16184.9246\n",
            "Epoch 5/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 65.2088 - mean_squared_error: 14849.8712 - val_loss: 70.3914 - val_mean_squared_error: 17011.5634\n",
            "Epoch 6/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 66.0173 - mean_squared_error: 15639.2136 - val_loss: 73.3304 - val_mean_squared_error: 15458.4673\n",
            "Epoch 7/100\n",
            "4180/4180 [==============================] - 1s 221us/step - loss: 65.4391 - mean_squared_error: 15004.8579 - val_loss: 78.2634 - val_mean_squared_error: 17945.3553\n",
            "Epoch 8/100\n",
            "4180/4180 [==============================] - 1s 223us/step - loss: 65.0829 - mean_squared_error: 14709.7951 - val_loss: 69.6878 - val_mean_squared_error: 16634.8233\n",
            "Epoch 9/100\n",
            "4180/4180 [==============================] - 1s 219us/step - loss: 65.8396 - mean_squared_error: 17343.7672 - val_loss: 75.4913 - val_mean_squared_error: 19600.5580\n",
            "Epoch 10/100\n",
            "4180/4180 [==============================] - 1s 229us/step - loss: 65.2713 - mean_squared_error: 14461.5735 - val_loss: 75.9204 - val_mean_squared_error: 17582.3081\n",
            "Epoch 11/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 64.3142 - mean_squared_error: 14488.7825 - val_loss: 75.2417 - val_mean_squared_error: 15139.7898\n",
            "Epoch 12/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 63.8907 - mean_squared_error: 14614.5193 - val_loss: 73.3540 - val_mean_squared_error: 16961.1719\n",
            "Epoch 13/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 63.2875 - mean_squared_error: 14374.6015 - val_loss: 70.7790 - val_mean_squared_error: 17764.4949\n",
            "Epoch 14/100\n",
            "4180/4180 [==============================] - 1s 220us/step - loss: 62.3921 - mean_squared_error: 13917.6454 - val_loss: 70.1878 - val_mean_squared_error: 15500.4741\n",
            "Epoch 15/100\n",
            "4180/4180 [==============================] - 1s 223us/step - loss: 63.1363 - mean_squared_error: 13793.4471 - val_loss: 70.0165 - val_mean_squared_error: 15428.4716\n",
            "Epoch 16/100\n",
            "4180/4180 [==============================] - 1s 215us/step - loss: 61.9928 - mean_squared_error: 13848.2691 - val_loss: 69.7624 - val_mean_squared_error: 14921.0625\n",
            "Epoch 17/100\n",
            "4180/4180 [==============================] - 1s 223us/step - loss: 61.3632 - mean_squared_error: 13481.6444 - val_loss: 67.2681 - val_mean_squared_error: 14064.4419\n",
            "Epoch 18/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 61.2378 - mean_squared_error: 13120.9874 - val_loss: 69.2963 - val_mean_squared_error: 14879.9221\n",
            "Epoch 19/100\n",
            "4180/4180 [==============================] - 1s 231us/step - loss: 61.5789 - mean_squared_error: 13850.0047 - val_loss: 68.1737 - val_mean_squared_error: 15620.0248\n",
            "Epoch 20/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 61.6115 - mean_squared_error: 13377.7577 - val_loss: 68.2468 - val_mean_squared_error: 14006.6353\n",
            "Epoch 21/100\n",
            "4180/4180 [==============================] - 1s 241us/step - loss: 61.6209 - mean_squared_error: 13878.4257 - val_loss: 70.1398 - val_mean_squared_error: 14577.4270\n",
            "Epoch 22/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 60.8938 - mean_squared_error: 13275.7016 - val_loss: 79.6041 - val_mean_squared_error: 16450.9690\n",
            "Epoch 23/100\n",
            "4180/4180 [==============================] - 1s 245us/step - loss: 61.7256 - mean_squared_error: 13341.4075 - val_loss: 66.5117 - val_mean_squared_error: 15549.5250\n",
            "Epoch 24/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 60.5136 - mean_squared_error: 12983.6399 - val_loss: 68.1930 - val_mean_squared_error: 16643.8494\n",
            "Epoch 25/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 61.2284 - mean_squared_error: 13291.7390 - val_loss: 74.0702 - val_mean_squared_error: 15587.6140\n",
            "Epoch 26/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 61.5878 - mean_squared_error: 13343.7874 - val_loss: 73.2898 - val_mean_squared_error: 15625.6093\n",
            "Epoch 27/100\n",
            "4180/4180 [==============================] - 1s 242us/step - loss: 61.1760 - mean_squared_error: 13092.2451 - val_loss: 65.9606 - val_mean_squared_error: 14292.4339\n",
            "Epoch 28/100\n",
            "4180/4180 [==============================] - 1s 243us/step - loss: 60.7026 - mean_squared_error: 13200.7239 - val_loss: 70.1064 - val_mean_squared_error: 17089.5836\n",
            "Epoch 29/100\n",
            "4180/4180 [==============================] - 1s 227us/step - loss: 61.2176 - mean_squared_error: 13740.8225 - val_loss: 67.5152 - val_mean_squared_error: 15560.3869\n",
            "Epoch 30/100\n",
            "4180/4180 [==============================] - 1s 216us/step - loss: 60.1967 - mean_squared_error: 12831.4804 - val_loss: 65.1268 - val_mean_squared_error: 14298.7325\n",
            "Epoch 31/100\n",
            "4180/4180 [==============================] - 1s 216us/step - loss: 59.7516 - mean_squared_error: 12478.5518 - val_loss: 67.7006 - val_mean_squared_error: 15357.7022\n",
            "Epoch 32/100\n",
            "4180/4180 [==============================] - 1s 223us/step - loss: 60.1769 - mean_squared_error: 12775.7023 - val_loss: 71.4351 - val_mean_squared_error: 14441.9038\n",
            "Epoch 33/100\n",
            "4180/4180 [==============================] - 1s 227us/step - loss: 59.9440 - mean_squared_error: 12741.2870 - val_loss: 65.4608 - val_mean_squared_error: 14290.7824\n",
            "Epoch 34/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 59.2937 - mean_squared_error: 12657.3273 - val_loss: 70.7525 - val_mean_squared_error: 15254.0566\n",
            "Epoch 35/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 59.5008 - mean_squared_error: 12564.2065 - val_loss: 66.5194 - val_mean_squared_error: 15135.8680\n",
            "Epoch 36/100\n",
            "4180/4180 [==============================] - 1s 233us/step - loss: 60.2118 - mean_squared_error: 12580.7626 - val_loss: 69.5568 - val_mean_squared_error: 13606.8848\n",
            "Epoch 37/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 60.1033 - mean_squared_error: 12859.4976 - val_loss: 73.5383 - val_mean_squared_error: 14294.4272\n",
            "Epoch 38/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 60.6080 - mean_squared_error: 12808.2805 - val_loss: 66.4620 - val_mean_squared_error: 14878.8303\n",
            "Epoch 39/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 59.8748 - mean_squared_error: 12693.2855 - val_loss: 65.9784 - val_mean_squared_error: 14437.2910\n",
            "Epoch 40/100\n",
            "4180/4180 [==============================] - 1s 235us/step - loss: 58.7562 - mean_squared_error: 12575.2828 - val_loss: 66.0595 - val_mean_squared_error: 14396.5703\n",
            "Epoch 41/100\n",
            "4180/4180 [==============================] - 1s 223us/step - loss: 59.2684 - mean_squared_error: 12135.3625 - val_loss: 66.2382 - val_mean_squared_error: 14556.0914\n",
            "Epoch 42/100\n",
            "4180/4180 [==============================] - 1s 226us/step - loss: 59.6247 - mean_squared_error: 12636.9852 - val_loss: 68.1846 - val_mean_squared_error: 14624.6311\n",
            "Epoch 43/100\n",
            "4180/4180 [==============================] - 1s 226us/step - loss: 58.3145 - mean_squared_error: 12288.1663 - val_loss: 66.7927 - val_mean_squared_error: 13270.5464\n",
            "Epoch 44/100\n",
            "4180/4180 [==============================] - 1s 212us/step - loss: 58.7865 - mean_squared_error: 12288.4009 - val_loss: 64.9183 - val_mean_squared_error: 14701.5763\n",
            "Epoch 45/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 58.6677 - mean_squared_error: 12324.7131 - val_loss: 65.5500 - val_mean_squared_error: 13792.5934\n",
            "Epoch 46/100\n",
            "4180/4180 [==============================] - 1s 225us/step - loss: 58.6112 - mean_squared_error: 12589.7228 - val_loss: 63.9473 - val_mean_squared_error: 13945.5124\n",
            "Epoch 47/100\n",
            "4180/4180 [==============================] - 1s 222us/step - loss: 59.4327 - mean_squared_error: 12807.5481 - val_loss: 64.5573 - val_mean_squared_error: 14199.3078\n",
            "Epoch 48/100\n",
            "4180/4180 [==============================] - 1s 218us/step - loss: 58.5170 - mean_squared_error: 12018.7415 - val_loss: 67.2921 - val_mean_squared_error: 13809.9769\n",
            "Epoch 49/100\n",
            "4180/4180 [==============================] - 1s 215us/step - loss: 59.1288 - mean_squared_error: 12534.6871 - val_loss: 67.8965 - val_mean_squared_error: 14563.3857\n",
            "Epoch 50/100\n",
            "4180/4180 [==============================] - 1s 226us/step - loss: 58.9878 - mean_squared_error: 12606.9414 - val_loss: 64.1098 - val_mean_squared_error: 13568.2123\n",
            "Epoch 51/100\n",
            "4180/4180 [==============================] - 1s 217us/step - loss: 58.8918 - mean_squared_error: 12384.4948 - val_loss: 64.9331 - val_mean_squared_error: 14688.6655\n",
            "Epoch 52/100\n",
            "4180/4180 [==============================] - 1s 216us/step - loss: 58.8429 - mean_squared_error: 12532.6294 - val_loss: 65.7832 - val_mean_squared_error: 14229.3057\n",
            "Epoch 53/100\n",
            "4180/4180 [==============================] - 1s 213us/step - loss: 59.0450 - mean_squared_error: 12404.4445 - val_loss: 65.5501 - val_mean_squared_error: 14658.8849\n",
            "Epoch 54/100\n",
            "4180/4180 [==============================] - 1s 219us/step - loss: 58.8606 - mean_squared_error: 12548.5632 - val_loss: 66.3927 - val_mean_squared_error: 17157.4781\n",
            "Epoch 55/100\n",
            "4180/4180 [==============================] - 1s 239us/step - loss: 58.4607 - mean_squared_error: 12445.3143 - val_loss: 73.1642 - val_mean_squared_error: 17378.5535\n",
            "Epoch 56/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 59.4523 - mean_squared_error: 12664.1977 - val_loss: 69.0702 - val_mean_squared_error: 13562.8974\n",
            "Epoch 57/100\n",
            "4180/4180 [==============================] - 1s 229us/step - loss: 58.9076 - mean_squared_error: 12213.8086 - val_loss: 67.1689 - val_mean_squared_error: 16490.2198\n",
            "Epoch 58/100\n",
            "4180/4180 [==============================] - 1s 236us/step - loss: 58.6990 - mean_squared_error: 12186.2384 - val_loss: 65.3673 - val_mean_squared_error: 13530.8052\n",
            "Epoch 59/100\n",
            "4180/4180 [==============================] - 1s 238us/step - loss: 58.2652 - mean_squared_error: 12224.0993 - val_loss: 73.0642 - val_mean_squared_error: 20494.5491\n",
            "Epoch 60/100\n",
            "4180/4180 [==============================] - 1s 245us/step - loss: 59.2819 - mean_squared_error: 12587.4282 - val_loss: 77.5906 - val_mean_squared_error: 17463.3029\n",
            "Epoch 61/100\n",
            "4180/4180 [==============================] - 1s 220us/step - loss: 58.2647 - mean_squared_error: 12398.5682 - val_loss: 64.1013 - val_mean_squared_error: 14129.3503\n",
            "14987.618441868917\n",
            "Finished TreeExplainer\n",
            "len one 34\n",
            "len two 34\n",
            "len one 34\n",
            "len two 34\n",
            "final\n",
            "Feature shape: (205,)\n",
            "Train on 4180 samples, validate on 1046 samples\n",
            "Epoch 1/100\n",
            "4180/4180 [==============================] - 4s 970us/step - loss: 2642.2557 - mean_squared_error: 947828854.9098 - val_loss: 181.0047 - val_mean_squared_error: 283082.6485\n",
            "Epoch 2/100\n",
            "4180/4180 [==============================] - 1s 229us/step - loss: 147.6794 - mean_squared_error: 69287.5937 - val_loss: 149.6875 - val_mean_squared_error: 57419.9678\n",
            "Epoch 3/100\n",
            "4180/4180 [==============================] - 1s 222us/step - loss: 140.3294 - mean_squared_error: 51786.4754 - val_loss: 147.8914 - val_mean_squared_error: 57822.0148\n",
            "Epoch 4/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 126.1076 - mean_squared_error: 46244.3286 - val_loss: 116.6058 - val_mean_squared_error: 49729.5472\n",
            "Epoch 5/100\n",
            "4180/4180 [==============================] - 1s 244us/step - loss: 116.9073 - mean_squared_error: 41899.3641 - val_loss: 200.7725 - val_mean_squared_error: 384806.9334\n",
            "Epoch 6/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 121.2441 - mean_squared_error: 95397.1652 - val_loss: 147.5498 - val_mean_squared_error: 56708.4948\n",
            "Epoch 7/100\n",
            "4180/4180 [==============================] - 1s 227us/step - loss: 137.1124 - mean_squared_error: 48960.4913 - val_loss: 146.3495 - val_mean_squared_error: 56704.5563\n",
            "Epoch 8/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 131.4371 - mean_squared_error: 46595.1708 - val_loss: 140.0340 - val_mean_squared_error: 53257.5148\n",
            "Epoch 9/100\n",
            "4180/4180 [==============================] - 1s 230us/step - loss: 132.2650 - mean_squared_error: 52499.6953 - val_loss: 138.3997 - val_mean_squared_error: 53379.6131\n",
            "Epoch 10/100\n",
            "4180/4180 [==============================] - 1s 252us/step - loss: 126.4705 - mean_squared_error: 44319.4928 - val_loss: 135.4420 - val_mean_squared_error: 51474.1807\n",
            "Epoch 11/100\n",
            "4180/4180 [==============================] - 1s 260us/step - loss: 123.8164 - mean_squared_error: 43878.6068 - val_loss: 140.5732 - val_mean_squared_error: 53540.2326\n",
            "Epoch 12/100\n",
            "4180/4180 [==============================] - 1s 243us/step - loss: 121.4054 - mean_squared_error: 42344.4523 - val_loss: 129.5857 - val_mean_squared_error: 49517.4686\n",
            "Epoch 13/100\n",
            "4180/4180 [==============================] - 1s 247us/step - loss: 115.6043 - mean_squared_error: 40602.0257 - val_loss: 134.6083 - val_mean_squared_error: 51354.2353\n",
            "Epoch 14/100\n",
            "4180/4180 [==============================] - 1s 251us/step - loss: 118.4765 - mean_squared_error: 46016.4106 - val_loss: 136.5895 - val_mean_squared_error: 53438.1993\n",
            "Epoch 15/100\n",
            "4180/4180 [==============================] - 1s 250us/step - loss: 118.7122 - mean_squared_error: 41358.2344 - val_loss: 128.0578 - val_mean_squared_error: 48416.0206\n",
            "Epoch 16/100\n",
            "4180/4180 [==============================] - 1s 255us/step - loss: 115.6261 - mean_squared_error: 40324.7133 - val_loss: 124.1642 - val_mean_squared_error: 46884.7594\n",
            "Epoch 17/100\n",
            "4180/4180 [==============================] - 1s 224us/step - loss: 113.5690 - mean_squared_error: 40344.1788 - val_loss: 120.8039 - val_mean_squared_error: 45561.7139\n",
            "Epoch 18/100\n",
            "4180/4180 [==============================] - 1s 221us/step - loss: 110.9208 - mean_squared_error: 38769.5580 - val_loss: 120.4065 - val_mean_squared_error: 46036.1191\n",
            "Epoch 19/100\n",
            "4180/4180 [==============================] - 1s 234us/step - loss: 111.4735 - mean_squared_error: 39906.9081 - val_loss: 117.8237 - val_mean_squared_error: 44338.4966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rMAYjqbV8SS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "60caad3b-6277-470f-80e2-f0a16feeb48a"
      },
      "source": [
        "new_first_tree, new_second_tree, shapper_tree, shap_select_abs = runner(first, second, target, modeller=\"LightGBM\")"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final\n",
            "Finished TreeExplainer\n",
            "12950.388736699597\n",
            "Finished TreeExplainer\n",
            "len one 25\n",
            "len two 25\n",
            "len one 25\n",
            "len two 25\n",
            "final\n",
            "Finished TreeExplainer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk6KH0Nn4UlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cap = shapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHR5KafV-iZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shap_select_abs = shap_select_abs.sort_values(ascending=False)\n",
        "# shap_select_abs[shap_select_abs.cumsum().sub((shap_select_abs.sum()*0.9)).le(0)].shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeDAKg5G_7oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def all(new_first_two,new_second_two, shapper,target ):\n",
        "  def scaler(df):\n",
        "    x = df.values #returns a numpy array\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "    df = pd.DataFrame(x_scaled, index=df.index, columns=df.columns)\n",
        "    return df\n",
        "\n",
        "  shapper = new_first_two.columns\n",
        "  #shapper = cap\n",
        "\n",
        "  new_first_y = new_first_two[target]\n",
        "  new_first = new_first_two[shapper].copy()\n",
        "  new_first = scaler(new_first.drop([target],axis=1))\n",
        "\n",
        "  new_second_y = new_second_two[target]\n",
        "  new_second = new_second_two[shapper].copy()\n",
        "  new_second = scaler(new_second.drop([target],axis=1)) \n",
        "\n",
        "  return new_first,new_first_y, new_second, new_second_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6tLpv6zW0tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_tree_one,_, new_tree_two,_  = all(new_first_tree,new_second_tree, shapper_tree,target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xWS_ASQeHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_deep_one,new_first_y, new_deep_two, new_second_y = all(new_first_deep,new_second_deep, shapper_deep,target )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFDv1FvBY38J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_first =  pd.concat((new_tree_one,new_deep_one ),axis=1)\n",
        "new_second =  pd.concat((new_tree_two,new_deep_two ),axis=1)\n",
        "\n",
        "new_first = new_first.loc[:,~new_first.columns.duplicated()]\n",
        "new_first = new_first.loc[:,~new_first.columns.duplicated()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOT0UpGcOOF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "8d1ed5b7-c438-4033-95f5-4bba7fc14806"
      },
      "source": [
        "new_second.head()"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>review_scores_rating</th>\n",
              "      <th>number_of_reviews</th>\n",
              "      <th>minimum_nights</th>\n",
              "      <th>security_deposit</th>\n",
              "      <th>cleaning_fee</th>\n",
              "      <th>accommodates</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>beds</th>\n",
              "      <th>availability_365</th>\n",
              "      <th>host_identity_verified</th>\n",
              "      <th>host_is_superhost</th>\n",
              "      <th>bedrooms_per_person</th>\n",
              "      <th>bathrooms_per_person</th>\n",
              "      <th>past_and_future_popularity</th>\n",
              "      <th>Bondi</th>\n",
              "      <th>Bondi Beach</th>\n",
              "      <th>Coogee</th>\n",
              "      <th>Darlinghurst</th>\n",
              "      <th>Manly</th>\n",
              "      <th>North Bondi</th>\n",
              "      <th>Randwick</th>\n",
              "      <th>Redfern</th>\n",
              "      <th>Surry Hills</th>\n",
              "      <th>Sydney</th>\n",
              "      <th>Apartment</th>\n",
              "      <th>Bed and breakfast</th>\n",
              "      <th>Boutique hotel</th>\n",
              "      <th>Bungalow</th>\n",
              "      <th>Condominium</th>\n",
              "      <th>Cottage</th>\n",
              "      <th>Guest suite</th>\n",
              "      <th>Guesthouse</th>\n",
              "      <th>Hostel</th>\n",
              "      <th>Hotel</th>\n",
              "      <th>House</th>\n",
              "      <th>Loft</th>\n",
              "      <th>Other</th>\n",
              "      <th>...</th>\n",
              "      <th>security_deposit_RECIP_p_one</th>\n",
              "      <th>cleaning_fee_RECIP_p_one</th>\n",
              "      <th>Manly_RECIP_p_one</th>\n",
              "      <th>Private room_RECIP_p_one</th>\n",
              "      <th>Randwick_RECIP_p_one</th>\n",
              "      <th>bedrooms_RECIP_p_one</th>\n",
              "      <th>accommodates_RECIP_p_one</th>\n",
              "      <th>bathrooms_RECIP_p_one</th>\n",
              "      <th>Surry Hills_RECIP_p_one</th>\n",
              "      <th>Townhouse_RECIP_p_one</th>\n",
              "      <th>Sydney_RECIP_p_one</th>\n",
              "      <th>Boutique hotel_RECIP_p_one</th>\n",
              "      <th>Redfern_RECIP_p_one</th>\n",
              "      <th>Bondi_RECIP_p_one</th>\n",
              "      <th>North Bondi_RECIP_p_one</th>\n",
              "      <th>minimum_nights_RECIP_p_one</th>\n",
              "      <th>Entire home/apt_RECIP_p_one</th>\n",
              "      <th>Shared room_RECIP_p_one</th>\n",
              "      <th>Hotel room_RECIP_p_one</th>\n",
              "      <th>number_of_reviews_SQRT_p_one</th>\n",
              "      <th>availability_365_SQRT_p_one</th>\n",
              "      <th>security_deposit_SQRT_p_one</th>\n",
              "      <th>cleaning_fee_SQRT_p_one</th>\n",
              "      <th>Manly_SQRT_p_one</th>\n",
              "      <th>Private room_SQRT_p_one</th>\n",
              "      <th>Randwick_SQRT_p_one</th>\n",
              "      <th>bedrooms_SQRT_p_one</th>\n",
              "      <th>accommodates_SQRT_p_one</th>\n",
              "      <th>bathrooms_SQRT_p_one</th>\n",
              "      <th>Surry Hills_SQRT_p_one</th>\n",
              "      <th>Townhouse_SQRT_p_one</th>\n",
              "      <th>Sydney_SQRT_p_one</th>\n",
              "      <th>Boutique hotel_SQRT_p_one</th>\n",
              "      <th>Redfern_SQRT_p_one</th>\n",
              "      <th>Bondi_SQRT_p_one</th>\n",
              "      <th>North Bondi_SQRT_p_one</th>\n",
              "      <th>minimum_nights_SQRT_p_one</th>\n",
              "      <th>Entire home/apt_SQRT_p_one</th>\n",
              "      <th>Shared room_SQRT_p_one</th>\n",
              "      <th>Hotel room_SQRT_p_one</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.709262</td>\n",
              "      <td>0.598421</td>\n",
              "      <td>0.8500</td>\n",
              "      <td>0.651663</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0625</td>\n",
              "      <td>0.849315</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.665999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.798785</td>\n",
              "      <td>0.917494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.226541</td>\n",
              "      <td>0.117331</td>\n",
              "      <td>0.251687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010516</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.923012</td>\n",
              "      <td>0.570964</td>\n",
              "      <td>0.8250</td>\n",
              "      <td>0.048924</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>0.365764</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.731507</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.398799</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.189529</td>\n",
              "      <td>0.847752</td>\n",
              "      <td>0.600024</td>\n",
              "      <td>0.369161</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400372</td>\n",
              "      <td>0.303391</td>\n",
              "      <td>0.251687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027192</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.893442</td>\n",
              "      <td>0.581395</td>\n",
              "      <td>0.8625</td>\n",
              "      <td>0.021526</td>\n",
              "      <td>0.004004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.243836</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.008378</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.331999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.113934</td>\n",
              "      <td>0.468081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.369161</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400372</td>\n",
              "      <td>0.303391</td>\n",
              "      <td>0.251687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.034253</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.862675</td>\n",
              "      <td>0.486743</td>\n",
              "      <td>0.9750</td>\n",
              "      <td>0.025440</td>\n",
              "      <td>0.013013</td>\n",
              "      <td>0.438917</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>0.120548</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.072727</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.131598</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.126768</td>\n",
              "      <td>0.314829</td>\n",
              "      <td>0.658439</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792752</td>\n",
              "      <td>0.702284</td>\n",
              "      <td>0.880747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.081351</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.946738</td>\n",
              "      <td>0.569892</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>0.127202</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.073153</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.1875</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.230496</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001850</td>\n",
              "      <td>0.006107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.238095</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.665999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.329398</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.261790</td>\n",
              "      <td>0.426092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400372</td>\n",
              "      <td>0.303391</td>\n",
              "      <td>0.251687</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010516</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 352 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       longitude  latitude  ...  Shared room_SQRT_p_one  Hotel room_SQRT_p_one\n",
              "index                       ...                                               \n",
              "3       0.709262  0.598421  ...                     0.0                    0.0\n",
              "11      0.923012  0.570964  ...                     0.0                    0.0\n",
              "13      0.893442  0.581395  ...                     0.0                    0.0\n",
              "22      0.862675  0.486743  ...                     0.0                    0.0\n",
              "32      0.946738  0.569892  ...                     0.0                    0.0\n",
              "\n",
              "[5 rows x 352 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJYKNdncCij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b927384-dffa-4ce5-bcf4-87c28a529642"
      },
      "source": [
        "new_first.shape"
      ],
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5226, 56)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK1gwIsMTpG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ff33458-a33a-43aa-d4d3-7ec30d2fa925"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Lasso(alpha=0.15, tol =0.001)\n",
        "lass = clf.fit(new_first,new_first_y)\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "# #12381 0.10\n"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13472.848965713389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPLdLcD9SfYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1eef9ddd-4c71-4aac-8df3-e8e106b6c39b"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Lasso(alpha=0.25, tol =0.001)\n",
        "lass = clf.fit(new_first,new_first_y)\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "# #12381 0.10\n"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12883.485802884878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLHRmDl0Ro5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e96db94a-ee33-4d0b-fc89-14fd3f2ee819"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Lasso(alpha=0.16, tol =0.001)\n",
        "lass = clf.fit(new_first,new_first_y)\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "# #12381 0.10\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12778.874244915849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDQ5rlSHD-d6",
        "colab_type": "code",
        "outputId": "51fc5561-5bcd-4c82-80d3-878c1db2c1fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Lasso(alpha=0.10, tol =0.001)\n",
        "lass = clf.fit(new_first,new_first_y)\n",
        "preds = lass.predict(new_second)\n",
        "mse = mean_squared_error(new_second_y, preds)\n",
        "print(mse)\n",
        "\n",
        "# #12381 0.10\n"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12825.938490546223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQER9SJr21Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72a42595-ed3a-4c06-b5e9-586c8fdbe9ef"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5226, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZe1LpaL2373",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b984330-6cf4-4adb-cb8a-b4de007528ae"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5226, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_RV5vUeEw03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "# max_features\n",
        "model = SelectFromModel(lass, prefit=True)\n",
        "X_train = new_first[new_first.columns[model.get_support(indices=True)]]\n",
        "new_first_y = new_first_two[target]\n",
        "\n",
        "X_test = new_second[list(X_train.columns)]\n",
        "new_second_y = new_second_two[target]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b09-ZWLeN1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0ec29c9c-76a8-43f0-a049-c6ac5e67c9be"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5226, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpNj82fIT6aW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "7e8da4b5-8428-40a3-9e94-44261ada100a"
      },
      "source": [
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train.values,new_first_y.values)\n",
        "preds = lm.predict(X_test.values)\n",
        "\n",
        "mse = mean_squared_error(new_second_y.values, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-298-f9bacb0ae598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_first_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_second_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 206\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (5226,256) and (156,) not aligned: 256 (dim 1) != 156 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQsp9hCLSRfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf2c318f-648f-4951-94b1-a9d1462ff4b2"
      },
      "source": [
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train.values,new_first_y.values)\n",
        "preds = lm.predict(X_test.values)\n",
        "\n",
        "mse = mean_squared_error(new_second_y.values, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12459.541928470688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcoJyfK7R6o4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fb0c6573-f997-4a4b-c2b2-1d4e5ed59481"
      },
      "source": [
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train.values,new_first_y.values)\n",
        "preds = lm.predict(X_test.values)\n",
        "\n",
        "mse = mean_squared_error(new_second_y.values, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12474.432454149033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xFB60E4d4cb",
        "colab_type": "code",
        "outputId": "e294740d-b2a1-4a29-a6b1-2153e01d6ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "lm = linear_model.LinearRegression()\n",
        "lm = lm.fit(X_train.values,new_first_y.values)\n",
        "preds = lm.predict(X_test.values)\n",
        "\n",
        "mse = mean_squared_error(new_second_y.values, preds)\n",
        "print(mse)\n",
        "\n",
        "#12381 0.10"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12630.511915306992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdtY_TLvB_45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def scaler(df):\n",
        "  x = df.values #returns a numpy array\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df = pd.DataFrame(x_scaled)\n",
        "  return df\n",
        "\n",
        "add_first_y = first[target]\n",
        "add_first = scaler(first.drop([target],axis=1))\n",
        "\n",
        "add_second_y = second[target]\n",
        "add_second = scaler(second.drop([target],axis=1)) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06e-K1qFDYwC",
        "colab_type": "code",
        "outputId": "0a8dd276-c910-4894-a32f-81edeeeb8b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "#clf = linear_model.Lasso(alpha=0.4)\n",
        "clf = linear_model.LinearRegression()\n",
        "preds = clf.fit(add_first,add_first_y).predict(add_second)\n",
        "mse = mean_squared_error(add_second_y, preds)\n",
        "print(mse)\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16069.285865192336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Vzp5TYaxe7",
        "colab_type": "code",
        "outputId": "a135aeec-beb5-42c9-caa6-c859f09f49ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "X_train.columns"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['accommodates_/_past_and_future_popularity', 'bathrooms_per_person',\n",
              "       'bathrooms_X_longitude_X_Entire home/apt_X_bedrooms_X_accommodates_/_past_and_future_popularity_X_bathrooms_X_longitude_X_accommodates_X_bedrooms',\n",
              "       'Entire home/apt',\n",
              "       'accommodates_/_minimum_nights_X_longitude_X_minimum_nights',\n",
              "       'longitude_/_minimum_nights', 'bathrooms_X_longitude',\n",
              "       'Entire home/apt_X_bedrooms_/_bedrooms_X_longitude',\n",
              "       'number_of_reviews',\n",
              "       'bathrooms_X_longitude_X_Entire home/apt_X_bedrooms_recip_p_one',\n",
              "       'Entire home/apt_/_bedrooms',\n",
              "       'accommodates_X_bedrooms_/_longitude_X_minimum_nights',\n",
              "       'accommodates_/_minimum_nights_/_longitude_X_minimum_nights_recip_p_one',\n",
              "       'bathrooms_X_longitude_X_accommodates_X_security_deposit',\n",
              "       'past_and_future_popularity_/_number_of_reviews', 'longitude',\n",
              "       'review_scores_rating', 'availability_365',\n",
              "       'bathrooms_X_longitude_X_Entire home/apt_X_bedrooms_X_Entire home/apt_X_bedrooms_/_bedrooms_X_longitude',\n",
              "       'bedrooms_X_longitude_/_accommodates_X_bathrooms',\n",
              "       'bathrooms_X_longitude_X_Entire home/apt_X_bedrooms_/_Entire home/apt_X_bedrooms_/_bedrooms_X_longitude_recip_p_one',\n",
              "       'bathrooms_X_longitude_X_Entire home/apt_X_bedrooms_X_accommodates_/_past_and_future_popularity_/_bathrooms_X_longitude_X_accommodates_X_bedrooms',\n",
              "       'Entire home/apt_X_bedrooms_/_accommodates_X_security_deposit',\n",
              "       'accommodates_/_minimum_nights',\n",
              "       'accommodates_X_bathrooms_/_accommodates_X_security_deposit',\n",
              "       'bathrooms_X_longitude_/_accommodates_X_security_deposit', 'Sydney',\n",
              "       'bathrooms_X_longitude_/_bathrooms_per_person_/_past_and_future_popularity',\n",
              "       'cleaning_fee',\n",
              "       'accommodates_X_bedrooms_/_bedrooms_X_past_and_future_popularity',\n",
              "       'longitude_recip_p_one', 'bathrooms_X_past_and_future_popularity',\n",
              "       'Boutique hotel', 'Shared room',\n",
              "       'bedrooms_X_longitude_X_accommodates_X_bedrooms_recip_p_one',\n",
              "       'accommodates_X_past_and_future_popularity',\n",
              "       'bedrooms_/_past_and_future_popularity',\n",
              "       'accommodates_X_bedrooms_/_Entire home/apt_/_bedrooms',\n",
              "       'bedrooms_X_security_deposit',\n",
              "       'Entire home/apt_X_bedrooms_X_accommodates_X_minimum_nights',\n",
              "       'bedrooms_/_security_deposit', 'latitude',\n",
              "       'accommodates_X_security_deposit_recip_p_one',\n",
              "       'bedrooms_X_past_and_future_popularity_recip_p_one',\n",
              "       'bathrooms_X_security_deposit'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJTgfpVydapZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_first_two"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKIJMWVHbdVR",
        "colab_type": "code",
        "outputId": "2ac9a6c4-2cea-4856-81d2-5c0fd39e14fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "list(new_first.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "['bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_accommodates_/_past_and_future_popularity', 'accommodates_/_latitude_/_Entire home/apt_recip_p_one', 'accommodates_/_latitude_/_Entire home/apt_recip_p_one', 'bedrooms_X_bathrooms_X_longitude', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_recip_p_one', 'bedrooms_X_bathrooms_/_longitude', 'accommodates_X_past_and_future_popularity', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms', 'security_deposit_X_latitude', 'bathrooms_per_person_/_security_deposit', 'accommodates_/_latitude_/_Entire home/apt', 'bedrooms_X_Entire home/apt_/_longitude', 'accommodates_/_past_and_future_popularity_recip_p_one', 'bedrooms_X_Entire home/apt_/_bathrooms_/_past_and_future_popularity', 'bedrooms_X_Entire home/apt_/_accommodates_X_security_deposit', 'bedrooms_X_bathrooms_X_longitude_X_bedrooms_X_Entire home/apt_/_accommodates_/_latitude', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_accommodates_X_security_deposit_recip_p_one', 'bathrooms_X_past_and_future_popularity', 'bedrooms_X_Entire home/apt_X_accommodates_/_latitude', 'bedrooms_X_latitude', 'accommodates_/_latitude_X_bathrooms_/_past_and_future_popularity', 'bathrooms_/_latitude', 'bathrooms_X_security_deposit_recip_p_one', 'accommodates_/_latitude_X_Entire home/apt', 'bedrooms_X_bathrooms_X_longitude_recip_p_one', 'accommodates_/_latitude_/_bedrooms_/_latitude', 'number_of_reviews', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_bedrooms_/_past_and_future_popularity', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude', 'review_scores_rating', 'accommodates_/_past_and_future_popularity', 'cleaning_fee', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_recip_p_one', 'bedrooms_X_Entire home/apt_/_bedrooms_X_bathrooms', 'bathrooms_/_past_and_future_popularity_X_accommodates_X_security_deposit', 'bathrooms_/_past_and_future_popularity_/_accommodates_X_security_deposit', 'bathrooms_/_past_and_future_popularity', 'bedrooms_X_Entire home/apt_X_bathrooms_X_accommodates', 'availability_365', 'accommodates_/_latitude_X_bedrooms_/_latitude', 'bedrooms_X_bathrooms_X_longitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms', 'security_deposit_/_past_and_future_popularity', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_accommodates_/_past_and_future_popularity', 'bedrooms_X_bathrooms_X_longitude_/_bedrooms_X_Entire home/apt_/_accommodates_/_latitude', 'bedrooms_/_past_and_future_popularity_recip_p_one', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_/_accommodates_X_security_deposit_recip_p_one', 'security_deposit_/_latitude', 'bedrooms_X_past_and_future_popularity', 'bedrooms_/_past_and_future_popularity', 'bedrooms_/_latitude', 'longitude', 'bedrooms_X_Entire home/apt_X_longitude', 'bedrooms_X_Entire home/apt_X_accommodates_X_security_deposit', 'Boutique hotel', 'strict_14_with_grace_period', 'bedrooms_/_past_and_future_popularity_power_2', 'accommodates_X_security_deposit', 'House', 'latitude', 'accommodates_X_security_deposit_recip_p_one', 'bathrooms_X_accommodates', 'bedrooms_X_Entire home/apt_X_bathrooms_/_past_and_future_popularity', 'Shared room', 'bathrooms_/_security_deposit', 'bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms', 'bedrooms_X_bathrooms_X_longitude_/_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms', 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_/_accommodates_/_past_and_future_popularity', 'bathrooms_per_person_X_security_deposit', 'bathrooms_/_accommodates', 'bathrooms_X_latitude', 'Private room', 'accommodates_/_latitude_power_2']"
            ],
            "text/plain": [
              "['bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_accommodates_/_past_and_future_popularity',\n",
              " 'accommodates_/_latitude_/_Entire home/apt_recip_p_one',\n",
              " 'accommodates_/_latitude_/_Entire home/apt_recip_p_one',\n",
              " 'bedrooms_X_bathrooms_X_longitude',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_recip_p_one',\n",
              " 'bedrooms_X_bathrooms_/_longitude',\n",
              " 'accommodates_X_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms',\n",
              " 'security_deposit_X_latitude',\n",
              " 'bathrooms_per_person_/_security_deposit',\n",
              " 'accommodates_/_latitude_/_Entire home/apt',\n",
              " 'bedrooms_X_Entire home/apt_/_longitude',\n",
              " 'accommodates_/_past_and_future_popularity_recip_p_one',\n",
              " 'bedrooms_X_Entire home/apt_/_bathrooms_/_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_X_security_deposit',\n",
              " 'bedrooms_X_bathrooms_X_longitude_X_bedrooms_X_Entire home/apt_/_accommodates_/_latitude',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_accommodates_X_security_deposit_recip_p_one',\n",
              " 'bathrooms_X_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_X_accommodates_/_latitude',\n",
              " 'bedrooms_X_latitude',\n",
              " 'accommodates_/_latitude_X_bathrooms_/_past_and_future_popularity',\n",
              " 'bathrooms_/_latitude',\n",
              " 'bathrooms_X_security_deposit_recip_p_one',\n",
              " 'accommodates_/_latitude_X_Entire home/apt',\n",
              " 'bedrooms_X_bathrooms_X_longitude_recip_p_one',\n",
              " 'accommodates_/_latitude_/_bedrooms_/_latitude',\n",
              " 'number_of_reviews',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_X_bedrooms_/_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude',\n",
              " 'review_scores_rating',\n",
              " 'accommodates_/_past_and_future_popularity',\n",
              " 'cleaning_fee',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_recip_p_one',\n",
              " 'bedrooms_X_Entire home/apt_/_bedrooms_X_bathrooms',\n",
              " 'bathrooms_/_past_and_future_popularity_X_accommodates_X_security_deposit',\n",
              " 'bathrooms_/_past_and_future_popularity_/_accommodates_X_security_deposit',\n",
              " 'bathrooms_/_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_X_bathrooms_X_accommodates',\n",
              " 'availability_365',\n",
              " 'accommodates_/_latitude_X_bedrooms_/_latitude',\n",
              " 'bedrooms_X_bathrooms_X_longitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms',\n",
              " 'security_deposit_/_past_and_future_popularity',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_accommodates_/_past_and_future_popularity',\n",
              " 'bedrooms_X_bathrooms_X_longitude_/_bedrooms_X_Entire home/apt_/_accommodates_/_latitude',\n",
              " 'bedrooms_/_past_and_future_popularity_recip_p_one',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_/_accommodates_X_security_deposit_recip_p_one',\n",
              " 'security_deposit_/_latitude',\n",
              " 'bedrooms_X_past_and_future_popularity',\n",
              " 'bedrooms_/_past_and_future_popularity',\n",
              " 'bedrooms_/_latitude',\n",
              " 'longitude',\n",
              " 'bedrooms_X_Entire home/apt_X_longitude',\n",
              " 'bedrooms_X_Entire home/apt_X_accommodates_X_security_deposit',\n",
              " 'Boutique hotel',\n",
              " 'strict_14_with_grace_period',\n",
              " 'bedrooms_/_past_and_future_popularity_power_2',\n",
              " 'accommodates_X_security_deposit',\n",
              " 'House',\n",
              " 'latitude',\n",
              " 'accommodates_X_security_deposit_recip_p_one',\n",
              " 'bathrooms_X_accommodates',\n",
              " 'bedrooms_X_Entire home/apt_X_bathrooms_/_past_and_future_popularity',\n",
              " 'Shared room',\n",
              " 'bathrooms_/_security_deposit',\n",
              " 'bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms',\n",
              " 'bedrooms_X_bathrooms_X_longitude_/_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms',\n",
              " 'bedrooms_X_Entire home/apt_/_accommodates_/_latitude_X_bedrooms_X_Entire home/apt_X_bedrooms_X_bathrooms_/_accommodates_/_past_and_future_popularity',\n",
              " 'bathrooms_per_person_X_security_deposit',\n",
              " 'bathrooms_/_accommodates',\n",
              " 'bathrooms_X_latitude',\n",
              " 'Private room',\n",
              " 'accommodates_/_latitude_power_2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWnWZjN88uSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "# poly_reg = PolynomialFeatures(degree=3)\n",
        "# PR = poly_reg.fit(add_first)\n",
        "\n",
        "# X_poly = PR.transform(add_first)\n",
        "# X_pred = PR.transform(add_second)\n",
        "\n",
        "# pol_reg = linear_model.LinearRegression()\n",
        "# pol_reg.fit(X_poly, add_first_y)\n",
        "# preds = pol_reg.predict(X_pred)\n",
        "# mse = mean_squared_error(add_second_y, preds)\n",
        "# print(mse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtixIRs6aucm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}